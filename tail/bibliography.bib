@misc{e3nn,
  author       = {Mario Geiger and
                  Tess Smidt and
                  Alby M. and
                  Benjamin Kurt Miller and
                  Wouter Boomsma and
                  Bradley Dice and
                  Kostiantyn Lapchevskyi and
                  Maurice Weiler and
                  Michał Tyszkiewicz and
                  Simon Batzner and
                  Martin Uhrin and
                  Jes Frellsen and
                  Nuri Jung and
                  Sophia Sanborn and
                  Josh Rackers and
                  Michael Bailey},
  title        = {Euclidean neural networks: e3nn},
  year         = 2020,
  publisher    = {Zenodo},
  version      = {0.3.5},
  doi          = {10.5281/zenodo.5292912},
  url          = {https://doi.org/10.5281/zenodo.5292912}
}

@book{0035701,
  author    = {L{\'{a}}szl{\'{o}} Gy{\"{o}}rfi and
               Michael Kohler and
               Adam Krzyzak and
               Harro Walk},
  title     = {A Distribution-Free Theory of Nonparametric Regression},
  series    = {Springer series in statistics},
  publisher = {Springer},
  year      = {2002},
  url       = {https://doi.org/10.1007/b97848},
  doi       = {10.1007/b97848},
  isbn      = {978-0-387-95441-7},
  timestamp = {Tue, 16 May 2017 14:01:34 +0200},
  biburl    = {https://dblp.org/rec/books/daglib/0035701.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Vapnik99,
  author    = {Vladimir Vapnik},
  title     = {An overview of statistical learning theory},
  journal   = {{IEEE} Trans. Neural Networks},
  volume    = {10},
  number    = {5},
  pages     = {988--999},
  year      = {1999},
  url       = {https://doi.org/10.1109/72.788640},
  doi       = {10.1109/72.788640},
  timestamp = {Wed, 14 Nov 2018 10:32:29 +0100},
  biburl    = {https://dblp.org/rec/journals/tnn/Vapnik99.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{CuckerS02,
  author    = {Felipe Cucker and
               Steve Smale},
  title     = {Best Choices for Regularization Parameters in Learning Theory: On
               the Bias-Variance Problem},
  journal   = {Found. Comput. Math.},
  volume    = {2},
  number    = {4},
  pages     = {413--428},
  year      = {2002},
  url       = {https://doi.org/10.1007/s102080010030},
  doi       = {10.1007/s102080010030},
  timestamp = {Fri, 13 Mar 2020 14:35:27 +0100},
  biburl    = {https://dblp.org/rec/journals/focm/CuckerS02.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%%%%%%% SGD > GD

@article{HLT2019,
  title={Control batch size and learning rate to generalize well: Theoretical and empirical evidence},
  author={He, Fengxiang and Liu, Tongliang and Tao, Dacheng},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={1143--1152},
  year={2019}
}

@article{JKABFSB2018,
  title={Three factors influencing minima in SGD},
  author={Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zac and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Storkey, Amos and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{SED2020,
  title={On the Generalization Benefit of Noise in Stochastic Gradient Descent},
  author={Smith, Samuel and Elsen, Erich and De, Soham},
  booktitle={International Conference on Machine Learning},
  pages={9058--9067},
  year={2020},
  organization={PMLR}
}

@inproceedings{JacotSSHG20,
  author    = {Arthur Jacot and
               Berfin Simsek and
               Francesco Spadaro and
               Cl{\'{e}}ment Hongler and
               Franck Gabriel},
  title     = {Implicit Regularization of Random Feature Models},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning,
               {ICML} 2020, 13-18 July 2020, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  pages     = {4631--4640},
  publisher = {{PMLR}},
  year      = {2020},
  url       = {http://proceedings.mlr.press/v119/jacot20a.html},
  timestamp = {Tue, 15 Dec 2020 17:40:19 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/JacotSSHG20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dAscoliRBK20,
  author    = {St{\'{e}}phane d'Ascoli and
               Maria Refinetti and
               Giulio Biroli and
               Florent Krzakala},
  title     = {Double Trouble in Double Descent: Bias and Variance(s) in the Lazy
               Regime},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning,
               {ICML} 2020, 13-18 July 2020, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  pages     = {2280--2290},
  publisher = {{PMLR}},
  year      = {2020},
  url       = {http://proceedings.mlr.press/v119/d-ascoli20a.html},
  timestamp = {Tue, 15 Dec 2020 17:40:18 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/dAscoliRBK20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Mei_2019_dd,
   title={The Generalization Error of Random Features Regression: Precise Asymptotics and the Double Descent Curve},
   ISSN={1097-0312},
   url={http://dx.doi.org/10.1002/CPA.22008},
   DOI={10.1002/cpa.22008},
   journal={Communications on Pure and Applied Mathematics},
   publisher={Wiley},
   author={Mei, Song and Montanari, Andrea},
   year={2021},
   month={Jun}
}


@inproceedings{MalachS21cnnvsfc,
  author    = {Eran Malach and
               Shai Shalev{-}Shwartz},
  title     = {Computational Separation Between Convolutional and Fully-Connected
               Networks},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=hkMoYYEkBoI},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/MalachS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ji21naturalgradient,
  author    = {Junqing Ji and
               Xiaojia Kong and
               Yajing Zhang and
               Tongle Xu and
               Jing Zhang},
  title     = {A new adaptive variable step size natural gradient {BSS} algorithm},
  journal   = {J. Intell. Fuzzy Syst.},
  volume    = {41},
  number    = {1},
  pages     = {57--68},
  year      = {2021},
  url       = {https://doi.org/10.3233/JIFS-200111},
  doi       = {10.3233/JIFS-200111},
  timestamp = {Fri, 10 Sep 2021 17:59:19 +0200},
  biburl    = {https://dblp.org/rec/journals/jifs/JiKZXZ21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{SmithL18,
  author    = {Samuel L. Smith and
               Quoc V. Le},
  title     = {A Bayesian Perspective on Generalization and Stochastic Gradient Descent},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=BJij4yg0Z},
  timestamp = {Thu, 25 Jul 2019 14:25:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/SmithL18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{KeskarMNST17,
  author    = {Nitish Shirish Keskar and
               Dheevatsa Mudigere and
               Jorge Nocedal and
               Mikhail Smelyanskiy and
               Ping Tak Peter Tang},
  title     = {On Large-Batch Training for Deep Learning: Generalization Gap and
               Sharp Minima},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=H1oyRlYgg},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/KeskarMNST17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sgd,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2236626},
 abstract = {Let M(x) denote the expected value at level x of the response to a certain experiment. M(x) is assumed to be a monotone function of x but is unknown to the experimenter, and it is desired to find the solution x = θ of the equation M(x) = α, where α is a given constant. We give a method for making successive experiments at levels x1,x2,⋯ in such a way that xn will tend to θ in probability.},
 author = {Herbert Robbins and Sutton Monro},
 journal = {The Annals of Mathematical Statistics},
 number = {3},
 pages = {400--407},
 publisher = {Institute of Mathematical Statistics},
 title = {A Stochastic Approximation Method},
 volume = {22},
 year = {1951}
}


@inproceedings{SmithDBD21,
  author    = {Samuel L. Smith and
               Benoit Dherin and
               David G. T. Barrett and
               Soham De},
  title     = {On the Origin of Implicit Regularization in Stochastic Gradient Descent},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=rq\_Qr0c1Hyo},
  timestamp = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/SmithDBD21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Roberts21sgd,
  author    = {Daniel A. Roberts},
  title     = {{SGD} Implicitly Regularizes Generalization Error},
  journal   = {CoRR},
  volume    = {abs/2104.04874},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.04874},
  eprinttype = {arXiv},
  eprint    = {2104.04874},
  timestamp = {Mon, 19 Apr 2021 16:45:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-04874.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ChaudhariS18,
  author    = {Pratik Chaudhari and
               Stefano Soatto},
  title     = {Stochastic Gradient Descent Performs Variational Inference, Converges
               to Limit Cycles for Deep Networks},
  booktitle = {2018 Information Theory and Applications Workshop, {ITA} 2018, San
               Diego, CA, USA, February 11-16, 2018},
  pages     = {1--10},
  publisher = {{IEEE}},
  year      = {2018},
  url       = {https://doi.org/10.1109/ITA.2018.8503224},
  doi       = {10.1109/ITA.2018.8503224},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/ita/ChaudhariS18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Martens10,
  author    = {James Martens},
  editor    = {Johannes F{\"{u}}rnkranz and
               Thorsten Joachims},
  title     = {Deep learning via Hessian-free optimization},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning
               (ICML-10), June 21-24, 2010, Haifa, Israel},
  pages     = {735--742},
  publisher = {Omnipress},
  year      = {2010},
  url       = {https://icml.cc/Conferences/2010/papers/458.pdf},
  timestamp = {Wed, 03 Apr 2019 17:43:36 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/Martens10.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{HeZRS15,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on
               ImageNet Classification},
  booktitle = {2015 {IEEE} International Conference on Computer Vision, {ICCV} 2015,
               Santiago, Chile, December 7-13, 2015},
  pages     = {1026--1034},
  publisher = {{IEEE} Computer Society},
  year      = {2015},
  url       = {https://doi.org/10.1109/ICCV.2015.123},
  doi       = {10.1109/ICCV.2015.123},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/iccv/HeZRS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{HuXP20orth,
  author    = {Wei Hu and
               Lechao Xiao and
               Jeffrey Pennington},
  title     = {Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear
               Networks},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=rkgqN1SYvr},
  timestamp = {Thu, 07 May 2020 17:11:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/HuXP20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{glorot10xavier,
  author    = {Xavier Glorot and
               Yoshua Bengio},
  editor    = {Yee Whye Teh and
               D. Mike Titterington},
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial
               Intelligence and Statistics, {AISTATS} 2010, Chia Laguna Resort, Sardinia,
               Italy, May 13-15, 2010},
  series    = {{JMLR} Proceedings},
  volume    = {9},
  pages     = {249--256},
  publisher = {JMLR.org},
  year      = {2010},
  url       = {http://proceedings.mlr.press/v9/glorot10a.html},
  timestamp = {Wed, 29 May 2019 08:41:47 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/GlorotB10.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{karras20gan,
  author    = {Tero Karras and
               Samuli Laine and
               Miika Aittala and
               Janne Hellsten and
               Jaakko Lehtinen and
               Timo Aila},
  title     = {Analyzing and Improving the Image Quality of StyleGAN},
  booktitle = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2020, Seattle, WA, USA, June 13-19, 2020},
  pages     = {8107--8116},
  publisher = {Computer Vision Foundation / {IEEE}},
  year      = {2020},
  url       = {https://openaccess.thecvf.com/content\_CVPR\_2020/html/Karras\_Analyzing\_and\_Improving\_the\_Image\_Quality\_of\_StyleGAN\_CVPR\_2020\_paper.html},
  doi       = {10.1109/CVPR42600.2020.00813},
  timestamp = {Tue, 31 Aug 2021 14:00:04 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/KarrasLAHLA20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{musenet,
  author    = {Alexandru Topirceanu and
               Gabriel Barina and
               Mihai Udrescu},
  title     = {MuSeNet: Collaboration in the Music Artists Industry},
  booktitle = {2014 European Network Intelligence Conference, {ENIC} 2014, Wroclaw,
               Poland, September 29-30, 2014},
  pages     = {89--94},
  publisher = {{IEEE} Computer Society},
  year      = {2014},
  url       = {https://doi.org/10.1109/ENIC.2014.10},
  doi       = {10.1109/ENIC.2014.10},
  timestamp = {Wed, 16 Oct 2019 14:14:56 +0200},
  biburl    = {https://dblp.org/rec/conf/enic/TopirceanuBU14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{colbois21face,
  author    = {Laurent Colbois and
               Tiago de Freitas Pereira and
               S{\'{e}}bastien Marcel},
  title     = {On the use of automatically generated synthetic image datasets for
               benchmarking face recognition},
  booktitle = {International {IEEE} Joint Conference on Biometrics, {IJCB} 2021,
               Shenzhen, China, August 4-7, 2021},
  pages     = {1--8},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/IJCB52358.2021.9484363},
  doi       = {10.1109/IJCB52358.2021.9484363},
  timestamp = {Fri, 23 Jul 2021 10:00:59 +0200},
  biburl    = {https://dblp.org/rec/conf/icb/ColboisPM21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{xiaohua21vit,
  author    = {Xiaohua Zhai and
               Alexander Kolesnikov and
               Neil Houlsby and
               Lucas Beyer},
  title     = {Scaling Vision Transformers},
  journal   = {CoRR},
  volume    = {abs/2106.04560},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.04560},
  eprinttype = {arXiv},
  eprint    = {2106.04560},
  timestamp = {Fri, 11 Jun 2021 11:04:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-04560.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{bronstein2021geometric,
    title={Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
    author={Michael M. Bronstein and Joan Bruna and Taco Cohen and Petar Veličković},
    year={2021},
    eprint={2104.13478},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@INPROCEEDINGS{worrall2017harmonic,  author={Worrall, Daniel E. and Garbin, Stephan J. and Turmukhambetov, Daniyar and Brostow, Gabriel J.},  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Harmonic Networks: Deep Translation and Rotation Equivariance},   year={2017},  volume={},  number={},  pages={7168-7177},  doi={10.1109/CVPR.2017.758}}

@article{smidt_2020, place={Cambridge}, title={Euclidean Symmetry and Equivariance in Machine Learning}, DOI={10.26434/chemrxiv.12935198.v1}, journal={ChemRxiv}, publisher={Cambridge Open Engage}, author={Smidt, Tess}, year={2020}} This content is a preprint and has not been peer-reviewed.

@article{smidt2021breaking,
  title = {Finding symmetry breaking order parameters with Euclidean neural networks},
  author = {Smidt, Tess E. and Geiger, Mario and Miller, Benjamin Kurt},
  journal = {Phys. Rev. Research},
  volume = {3},
  issue = {1},
  pages = {L012002},
  numpages = {6},
  year = {2021},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevResearch.3.L012002},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.3.L012002}
}

@misc{miller2020relevance,
    title={Relevance of Rotationally Equivariant Convolutions for Predicting Molecular Properties},
    author={Benjamin Kurt Miller and Mario Geiger and Tess E. Smidt and Frank Noé},
    year={2020},
    eprint={2008.08461},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Chen_2021,
   title={Direct Prediction of Phonon Density of States With Euclidean Neural Networks},
   volume={8},
   ISSN={2198-3844},
   url={http://dx.doi.org/10.1002/advs.202004214},
   DOI={10.1002/advs.202004214},
   number={12},
   journal={Advanced Science},
   publisher={Wiley},
   author={Chen, Zhantao and Andrejevic, Nina and Smidt, Tess and Ding, Zhiwei and Xu, Qian and Chi, Yen‐Ting and Nguyen, Quynh T. and Alatas, Ahmet and Kong, Jing and Li, Mingda},
   year={2021},
   month={Mar},
   pages={2004214}
}

@misc{bietti2021sample,
    title={On the Sample Complexity of Learning with Geometric Stability},
    author={Alberto Bietti and Luca Venturi and Joan Bruna},
    year={2021},
    eprint={2106.07148},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@inproceedings{
chaudhari2018stochastic,
title={Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks},
author={Pratik Chaudhari and Stefano Soatto},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HyWrIgW0W},
}

@misc{roberts2021sgd,
    title={SGD Implicitly Regularizes Generalization Error},
    author={Daniel A. Roberts},
    year={2021},
    eprint={2104.04874},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{
smith2021on,
title={On the Origin of Implicit Regularization in Stochastic Gradient Descent},
author={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=rq_Qr0c1Hyo}
}

@misc{nakkiran2019deep,
    title={Deep Double Descent: Where Bigger Models and More Data Hurt},
    author={Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever},
    year={2019},
    eprint={1912.02292},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{belkin2020twomodels,
    author = {Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
    title = {Two Models of Double Descent for Weak Features},
    journal = {SIAM Journal on Mathematics of Data Science},
    volume = {2},
    number = {4},
    pages = {1167-1180},
    year = {2020},
    doi = {10.1137/20M1336072},
    URL = {https://doi.org/10.1137/20M1336072},
    eprint = {https://doi.org/10.1137/20M1336072}
}


@InProceedings{pmlr-v48-cohenc16,
  title = 	 {Group Equivariant Convolutional Networks},
  author = 	 {Cohen, Taco and Welling, Max},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2990--2999},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/cohenc16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/cohenc16.html},
  abstract = 	 {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.}
}


@misc{thomas2018tensor,
    title={Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds},
    author={Nathaniel Thomas and Tess Smidt and Steven Kearnes and Lusann Yang and Li Li and Kai Kohlhoff and Patrick Riley},
    year={2018},
    eprint={1802.08219},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{fuchs2020se3transformers,
    title={SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks},
    author={Fabian B. Fuchs and Daniel E. Worrall and Volker Fischer and Max Welling},
    year={2020},
    eprint={2006.10503},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{yuan2021volo,
      title={VOLO: Vision Outlooker for Visual Recognition}, 
      author={Li Yuan and Qibin Hou and Zihang Jiang and Jiashi Feng and Shuicheng Yan},
      year={2021},
      eprint={2106.13112},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{golse2005hydrodynamic,
  title={Hydrodynamic limits},
  author={Golse, Fran{\c{c}}ois},
  booktitle={European Congress of Mathematics},
  pages={699--717},
  year={2005},
  organization={Eur Math Soc}
}

@misc{roberts2021principles,
    title={The Principles of Deep Learning Theory},
    author={Daniel A. Roberts and Sho Yaida and Boris Hanin},
    year={2021},
    eprint={2106.10165},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@Article{Popel2020,
author={Popel, Martin
and Tomkova, Marketa
and Tomek, Jakub
and Kaiser, {\L}ukasz
and Uszkoreit, Jakob
and Bojar, Ond{\v{r}}ej
and {\v{Z}}abokrtsk{\'y}, Zden{\v{e}}k},
title={Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals},
journal={Nature Communications},
year={2020},
month={Sep},
day={01},
volume={11},
number={1},
pages={4381},
abstract={The quality of human translation was long thought to be unattainable for computer translation systems. In this study, we present a deep-learning system, CUBBITT, which challenges this view. In a context-aware blind evaluation by human judges, CUBBITT significantly outperformed professional-agency English-to-Czech news translation in preserving text meaning (translation adequacy). While human translation is still rated as more fluent, CUBBITT is shown to be substantially more fluent than previous state-of-the-art systems. Moreover, most participants of a Translation Turing test struggle to distinguish CUBBITT translations from human translations. This work approaches the quality of human translation and even surpasses it in adequacy in certain circumstances.This suggests that deep learning may have the potential to replace humans in applications where conservation of meaning is the primary aim.},
issn={2041-1723},
doi={10.1038/s41467-020-18073-9},
url={https://doi.org/10.1038/s41467-020-18073-9}
}



@article{Mozaffari_2020,
   title={Deep Learning-Based Vehicle Behavior Prediction for Autonomous Driving Applications: A Review},
   ISSN={1558-0016},
   url={http://dx.doi.org/10.1109/TITS.2020.3012034},
   DOI={10.1109/tits.2020.3012034},
   journal={IEEE Transactions on Intelligent Transportation Systems},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Mozaffari, Sajjad and Al-Jarrah, Omar Y. and Dianati, Mehrdad and Jennings, Paul and Mouzakitis, Alexandros},
   year={2020},
   pages={1–15}
}

@book{rosenblatt,
    title={Principles of neurodynamics: Perceptron and the theory of brain mechanism},
    author={F. Rosenblatt},
    year={1961},
    publisher={Spartan Books. Washington, DC}
}

@book{hebb1949,
    title={The Organization of Behavior},
    author={D.O. Hebb},
    year={1949},
    publisher={Wiley: New York}
}

@misc{brown2020language,
    title={Language Models are Few-Shot Learners},
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year={2020},
    eprint={2005.14165},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article {Baekeabj8754,
	author = {Baek, Minkyung and DiMaio, Frank and Anishchenko, Ivan and Dauparas, Justas and Ovchinnikov, Sergey and Lee, Gyu Rie and Wang, Jue and Cong, Qian and Kinch, Lisa N. and Schaeffer, R. Dustin and Mill{\'a}n, Claudia and Park, Hahnbeom and Adams, Carson and Glassman, Caleb R. and DeGiovanni, Andy and Pereira, Jose H. and Rodrigues, Andria V. and van Dijk, Alberdina A. and Ebrecht, Ana C. and Opperman, Diederik J. and Sagmeister, Theo and Buhlheller, Christoph and Pavkov-Keller, Tea and Rathinaswamy, Manoj K. and Dalwadi, Udit and Yip, Calvin K. and Burke, John E. and Garcia, K. Christopher and Grishin, Nick V. and Adams, Paul D. and Read, Randy J. and Baker, David},
	title = {Accurate prediction of protein structures and interactions using a three-track neural network},
	elocation-id = {eabj8754},
	year = {2021},
	doi = {10.1126/science.abj8754},
	publisher = {American Association for the Advancement of Science},
	abstract = {DeepMind presented remarkably accurate predictions at the recent CASP14 protein structure prediction assessment conference. We explored network architectures incorporating related ideas and obtained the best performance with a three-track network in which information at the 1D sequence level, the 2D distance map level, and the 3D coordinate level is successively transformed and integrated. The three-track network produces structure predictions with accuracies approaching those of DeepMind in CASP14, enables the rapid solution of challenging X-ray crystallography and cryo-EM structure modeling problems, and provides insights into the functions of proteins of currently unknown structure. The network also enables rapid generation of accurate protein-protein complex models from sequence information alone, short circuiting traditional approaches which require modeling of individual subunits followed by docking. We make the method available to the scientific community to speed biological research.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/early/2021/07/19/science.abj8754},
	eprint = {https://science.sciencemag.org/content/early/2021/07/19/science.abj8754.full.pdf},
	journal = {Science}
}

@Article{Jumper2021,
author={Jumper, John
and Evans, Richard
and Pritzel, Alexander
and Green, Tim
and Figurnov, Michael
and Ronneberger, Olaf
and Tunyasuvunakool, Kathryn
and Bates, Russ
and {\v{Z}}{\'i}dek, Augustin
and Potapenko, Anna
and Bridgland, Alex
and Meyer, Clemens
and Kohl, Simon A. A.
and Ballard, Andrew J.
and Cowie, Andrew
and Romera-Paredes, Bernardino
and Nikolov, Stanislav
and Jain, Rishub
and Adler, Jonas
and Back, Trevor
and Petersen, Stig
and Reiman, David
and Clancy, Ellen
and Zielinski, Michal
and Steinegger, Martin
and Pacholska, Michalina
and Berghammer, Tamas
and Bodenstein, Sebastian
and Silver, David
and Vinyals, Oriol
and Senior, Andrew W.
and Kavukcuoglu, Koray
and Kohli, Pushmeet
and Hassabis, Demis},
title={Highly accurate protein structure prediction with AlphaFold},
journal={Nature},
year={2021},
month={Jul},
day={15},
abstract={Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1--4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the 3-D structure that a protein will adopt based solely on its amino acid sequence, the structure prediction component of the `protein folding problem'8, has been an important open research problem for more than 50 years9. Despite recent progress10--14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even where no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experiment in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
issn={1476-4687},
doi={10.1038/s41586-021-03819-2},
url={https://doi.org/10.1038/s41586-021-03819-2}
}


@article {Hopfield2554,
	author = {Hopfield, J J},
	title = {Neural networks and physical systems with emergent collective computational abilities},
	volume = {79},
	number = {8},
	pages = {2554--2558},
	year = {1982},
	doi = {10.1073/pnas.79.8.2554},
	publisher = {National Academy of Sciences},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/79/8/2554},
	eprint = {https://www.pnas.org/content/79/8/2554.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@misc{batzner2021se3equivariant,
      title={SE(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials}, 
      author={Simon Batzner and Albert Musaelian and Lixin Sun and Mario Geiger and Jonathan P. Mailoa and Mordechai Kornbluth and Nicola Molinari and Tess E. Smidt and Boris Kozinsky},
      year={2021},
      eprint={2101.03164},
      archivePrefix={arXiv},
      primaryClass={physics.comp-ph}
}

@misc{yang2020feature,
    title={Feature Learning in Infinite-Width Neural Networks},
    author={Greg Yang and Edward J. Hu},
    year={2020},
    eprint={2011.14522},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{yang2020tensor4,
  author    = {Greg Yang and
               Edward J. Hu},
  editor    = {Marina Meila and
               Tong Zhang},
  title     = {Tensor Programs {IV:} Feature Learning in Infinite-Width Neural Networks},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning,
               {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {11727--11737},
  publisher = {{PMLR}},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v139/yang21c.html},
  timestamp = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/YangH21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{chizat2020implicit,
	title = {Implicit {Bias} of {Gradient} {Descent} for {Wide} {Two}-layer {Neural} {Networks} {Trained} with the {Logistic} {Loss}},
	url = {http://proceedings.mlr.press/v125/chizat20a.html},
	abstract = {Neural networks trained to minimize the logistic (a.k.a. cross-entropy) loss with gradient-based methods are observed to perform well in many supervised classification tasks. Towards understanding...},
	language = {en},
	urldate = {2020-12-30},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Chizat, Lénaïc and Bach, Francis},
	month = jul,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1305--1338},
	file = {Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\4DGERV9X\\chizat20a.html:text/html;Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\HFQT8Q5V\\Chizat and Bach - 2020 - Implicit Bias of Gradient Descent for Wide Two-lay.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\D5Z4IIMI\\chizat20a.html:text/html},
}

@article{degiuli2015theory,
  title={Theory of the jamming transition at finite temperature},
  author={Degiuli, Eric and Lerner, E and Wyart, M},
  journal={The Journal of chemical physics},
  volume={142},
  number={16},
  pages={164503},
  year={2015},
  publisher={AIP Publishing LLC}
}

@inproceedings{jacot2020implicit,
	title = {Implicit {Regularization} of {Random} {Feature} {Models}},
	url = {http://proceedings.mlr.press/v119/jacot20a.html},
	abstract = {Random Features (RF) models are used as efficient parametric approximations of kernel methods. We investigate, by means of random matrix theory, the connection between Gaussian RF models and Kernel...},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jacot, Arthur and Simsek, Berfin and Spadaro, Francesco and Hongler, Clement and Gabriel, Franck},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {4631--4640},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\CMIQKW5B\\jacot20a.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\GKZRQTAQ\\Jacot et al. - 2020 - Implicit Regularization of Random Feature Models.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\HZFNA3AV\\jacot20a.html:text/html}
}

@article{chen2020dynamical,
	title = {A {Dynamical} {Central} {Limit} {Theorem} for {Shallow} {Neural} {Networks}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/fc5b3186f1cf0daece964f78259b7ba0-Abstract.html},
	language = {en},
	urldate = {2020-12-30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chen, Zhengdao and Rotskoff, Grant and Bruna, Joan and Vanden-Eijnden, Eric},
	year = {2020},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\8ED33A2T\\fc5b3186f1cf0daece964f78259b7ba0-Abstract.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\S3AF3PHK\\Chen et al. - 2020 - A Dynamical Central Limit Theorem for Shallow Neur.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\8YCRLAFY\\fc5b3186f1cf0daece964f78259b7ba0-Abstract.html:text/html}
}


@article{dyer2019asymptotics,
  title={Asymptotics of wide networks from feynman diagrams},
  author={Dyer, Ethan and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:1909.11304},
  year={2019}
}

@article{hanin2019finite,
  title={Finite depth and width corrections to the neural tangent kernel},
  author={Hanin, Boris and Nica, Mihai},
  journal={arXiv preprint arXiv:1909.05989},
  year={2019}
}
@article{franz2019critical,
  title={Critical jammed phase of the linear perceptron},
  author={Franz, Silvio and Sclocchi, Antonio and Urbani, Pierfrancesco},
  journal={Physical Review Letters},
  volume={123},
  number={11},
  pages={115702},
  year={2019},
  publisher={APS}
}
@article{wyart2010scaling,
  title={Scaling of phononic transport with connectivity in amorphous solids},
  author={Wyart, Matthieu},
  journal={EPL (Europhysics Letters)},
  volume={89},
  number={6},
  pages={64001},
  year={2010},
  publisher={IOP Publishing}
}
@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@article{wyart2012marginal,
  title={Marginal stability constrains force and pair distributions at random close packing},
  author={Wyart, Matthieu},
  journal={Physical review letters},
  volume={109},
  number={12},
  pages={125502},
  year={2012},
  publisher={APS}
}



@article{degiuli2015unified,
  title={Unified theory of inertial granular flows and non-Brownian suspensions},
  author={DeGiuli, E and D{\"u}ring, G and Lerner, E and Wyart, M},
  journal={Physical Review E},
  volume={91},
  number={6},
  pages={062206},
  year={2015},
  publisher={APS}
}

@misc{petrini2021relative,
      title={Relative stability toward diffeomorphisms indicates performance in deep nets}, 
      author={Leonardo Petrini and Alessandro Favero and Mario Geiger and Matthieu Wyart},
      year={2021},
      eprint={2105.02468},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{paccolat2020isotropic,
	title = {How isotropic kernels perform on simple invariants},
	issn = {2632-2153},
	url = {http://iopscience.iop.org/article/10.1088/2632-2153/abd485},
	doi = {10.1088/2632-2153/abd485},
	abstract = {We investigate how the training curve of isotropic kernel methods depends on the symmetry of the task to be learned, in several settings. (i) We consider a regression task, where the target function is a Gaussian random field that depends only on $^{\textrm{-{\textbackslash}beta\$}}$ where \$p\$ is the size of the training set. We find that \${\textbackslash}beta{\textbackslash}sim{\textbackslash}nicefrac1d\$ independently of \$d\_{\textbackslash}parallel\$, supporting previous findings that the presence of invariants does not resolve the curse of dimensionality for kernel regression. (ii) Next we consider support-vector binary classification and introduce the ıt stripe model where the data label depends on a single coordinate \$y({\textbackslash}vl x) = y(x\_1)\$, corresponding to parallel decision boundaries separating labels of different signs, and consider that there is no margin at these interfaces. We argue and confirm numerically that for large bandwidth, \${\textbackslash}beta = {\textbackslash}fracd-1+{\textbackslash}xi3d-3+{\textbackslash}xi\$, where \${\textbackslash}xiın (0,2)\$ is the exponent characterizing the singularity of the kernel at the origin. This estimation improves classical bounds obtainable from Rademacher complexity. In this setting there is no curse of dimensionality since \${\textbackslash}beta{\textbackslash}rightarrow {\textbackslash}nicefrac13\$ as \$d{\textbackslash}rightarrowınfty\$. (iii) We confirm these findings for the ıt spherical model for which \$y({\textbackslash}vl x) = y({\textbackslash}abs{\textbackslash}vl x)\$. (iv) In the stripe model, we show that if the data are compressed along their invariants by some factor λ (an operation believed to take place in deep networks), the test error is reduced by a factor \${\textbackslash}lambda{\textasciicircum}-{\textbackslash}frac2(d-1)3d-3+{\textbackslash}xi\$.},
	language = {en},
	urldate = {2020-12-30},
	journal = {Machine Learning: Science and Technology},
	author = {Paccolat, Jonas and Wyart, Matthieu and Spigler, Stefano},
	year = {2020},
	file = {IOP Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\CIYZAH4V\\Paccolat et al. - 2020 - How isotropic kernels perform on simple invariants.pdf:application/pdf}
}

@article{lerner2012unified,
  title={A unified framework for non-Brownian suspension flows and soft amorphous solids},
  author={Lerner, Edan and D{\"u}ring, Gustavo and Wyart, Matthieu},
  journal={Proceedings of the National Academy of Sciences},
  volume={109},
  number={13},
  pages={4798--4803},
  year={2012},
  publisher={National Acad Sciences}
}


@article{paccolat2020compressing,
  title={Geometric compression of invariant manifolds in neural nets},
  author={Paccolat, Jonas and Petrini, Leonardo and Geiger, Mario and Tyloo, Kevin and Wyart, Matthieu},
  journal={arXiv preprint arXiv:2007.11471},
  year={2020}
}


@article{saxe2019information,
  title={On the information bottleneck theory of deep learning},
  author={Saxe, Andrew M and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D and Cox, David D},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124020},
  year={2019},
  publisher={IOP Publishing}
}


@article{recanatesi2019dimensionality,
  title={Dimensionality compression and expansion in Deep Neural Networks},
  author={Recanatesi, Stefano and Farrell, Matthew and Advani, Madhu and Moore, Timothy and Lajoie, Guillaume and Shea-Brown, Eric},
  journal={arXiv preprint arXiv:1906.00443},
  year={2019}
}



@inproceedings{ansuini2019intrinsic,
  title={Intrinsic dimension of data representations in deep neural networks},
  author={Ansuini, Alessio and Laio, Alessandro and Macke, Jakob H and Zoccolan, Davide},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6111--6122},
  year={2019}
}


@article{mallat2016understanding,
  title={Understanding deep convolutional networks},
  author={Mallat, St{\'e}phane},
  journal={Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume={374},
  number={2065},
  pages={20150203},
  year={2016},
  publisher={The Royal Society Publishing}
}


@article{nguyen2020rigorous,
  title={A rigorous framework for the mean field limit of multilayer neural networks},
  author={Nguyen, Phan-Minh and Pham, Huy Tuan},
  journal={arXiv preprint arXiv:2001.11443},
  year={2020}
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={5th International Conference on Learning Representations, {ICLR} 2017,
              Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  year={2017}

}

@article{zhou2014object,
  title={Object detectors emerge in deep scene cnns},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  journal={arXiv preprint arXiv:1412.6856},
  year={2014}
}


@inproceedings{le2013building,
  title={Building high-level features using large scale unsupervised learning},
  author={Le, Quoc V},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={8595--8598},
  year={2013},
  organization={IEEE}
}

@article{maennel2018gradient,
  title={Gradient descent quantizes relu network features},
  author={Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
  journal={arXiv preprint arXiv:1803.08367},
  year={2018}
}


@article{fawzi2017robustness,
  title={The robustness of deep networks: A geometrical perspective},
  author={Fawzi, Alhussein and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={6},
  pages={50--62},
  year={2017},
  publisher={IEEE}
}


@inproceedings{stich2018sparsified,
  title={Sparsified SGD with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4447--4458},
  year={2018}
}

@inproceedings{gluch2020constructing,
  title={Constructing a provably adversarially-robust classifier from a high accuracy one},
  author={Gluch, Grzegorz and Urbanke, R{\"u}diger},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3674--3684},
  year={2020}
}


@article{abbe2020poly,
  title={Poly-time universality and limitations of deep learning},
  author={Abbe, Emmanuel and Sandon, Colin},
  journal={arXiv preprint arXiv:2001.02992},
  year={2020}
}

@inproceedings{goldt2019dynamics,
  title={Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup},
  author={Goldt, Sebastian and Advani, Madhu and Saxe, Andrew M and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6981--6991},
  year={2019}
}

@inproceedings{d2020double,
	title = {Double {Trouble} in {Double} {Descent}: {Bias} and {Variance}(s) in the {Lazy} {Regime}},
	shorttitle = {Double {Trouble} in {Double} {Descent}},
	url = {http://proceedings.mlr.press/v119/d-ascoli20a.html},
	abstract = {Deep neural networks can achieve remarkable generalization performances while interpolating the training data. Rather than the U-curve emblematic of the bias-variance trade-off, their test error of...},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {D’Ascoli, Stéphane and Refinetti, Maria and Biroli, Giulio and Krzakala, Florent},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {2280--2290},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\SUUJKK5I\\D’Ascoli et al. - 2020 - Double Trouble in Double Descent Bias and Varianc.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\6GZLNP5X\\d-ascoli20a.html:text/html}
}

@article{dietler2020yeaz,
  title={YeaZ: A convolutional neural network for highly accurate, label-free segmentation of yeast microscopy images},
  author={Dietler, Nicola and Minder, Matthias and Gligorovski, Vojislav and Economou, Augoustina Maria and Joly, Denis Alain Henri Lucien and Sadeghi, Ahmad and Chan, Chun Hei Michael and Kozi{\'n}ski, Mateusz and Weigert, Martin and Bitbol, Anne-Florence and others},
  journal={bioRxiv},
  year={2020},
  publisher={Cold Spring Harbor Laboratory}
}

@article{vstefko2018autonomous,
  title={Autonomous illumination control for localization microscopy},
  author={{\v{S}}tefko, Marcel and Ottino, Baptiste and Douglass, Kyle M and Manley, Suliana},
  journal={Optics express},
  volume={26},
  number={23},
  pages={30882--30900},
  year={2018},
  publisher={Optical Society of America}
}

@article{barbier2019adaptive,
  title={The adaptive interpolation method: a simple scheme to prove replica formulas in Bayesian inference},
  author={Barbier, Jean and Macris, Nicolas},
  journal={Probability theory and related fields},
  volume={174},
  number={3-4},
  pages={1133--1185},
  year={2019},
  publisher={Springer}
}

@article{de2016comparing,
  title={Comparing molecules and solids across structural and alchemical space},
  author={De, Sandip and Bart{\'o}k, Albert P and Cs{\'a}nyi, G{\'a}bor and Ceriotti, Michele},
  journal={Physical Chemistry Chemical Physics},
  volume={18},
  number={20},
  pages={13754--13769},
  year={2016},
  publisher={Royal Society of Chemistry}
}


@article{van2017learning,
  title={Learning phase transitions by confusion},
  author={Van Nieuwenburg, Evert PL and Liu, Ye-Hua and Huber, Sebastian D},
  journal={Nature Physics},
  volume={13},
  number={5},
  pages={435--439},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{carleo2017solving,
  title={Solving the quantum many-body problem with artificial neural networks},
  author={Carleo, Giuseppe and Troyer, Matthias},
  journal={Science},
  volume={355},
  number={6325},
  pages={602--606},
  year={2017},
  publisher={American Association for the Advancement of Science}
}


@article{huval2015empirical,
  title={An empirical evaluation of deep learning on highway driving},
  author={Huval, Brody and Wang, Tao and Tandon, Sameep and Kiske, Jeff and Song, Will and Pazhayampallil, Joel and Andriluka, Mykhaylo and Rajpurkar, Pranav and Migimatsu, Toki and Cheng-Yue, Royce and others},
  journal={arXiv preprint arXiv:1504.01716},
  year={2015}
}


@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{amodei2016deep,
  title={Deep speech 2: End-to-end speech recognition in english and mandarin},
  author={Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and others},
  booktitle={International conference on machine learning},
  pages={173--182},
  year={2016}
}


@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{shi2016end,
  title={An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition},
  author={Shi, Baoguang and Bai, Xiang and Yao, Cong},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={11},
  pages={2298--2304},
  year={2016},
  publisher={IEEE}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{marr1979computational,
  title={A computational theory of human stereo vision},
  author={Marr, David and Poggio, Tomaso},
  journal={Proceedings of the Royal Society of London. Series B. Biological Sciences},
  volume={204},
  number={1156},
  pages={301--328},
  year={1979},
  publisher={The Royal Society London}
}


@article{dieleman2016exploiting,
  title={Exploiting cyclic symmetry in convolutional neural networks},
  author={Dieleman, Sander and De Fauw, Jeffrey and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1602.02660},
  year={2016}
}

@article{zhang2019making,
  title={Making convolutional networks shift-invariant again},
  author={Zhang, Richard},
  journal={arXiv preprint arXiv:1904.11486},
  year={2019}
}


@article{azulay2018deep,
  title={Why do deep convolutional networks generalize so poorly to small image transformations?},
  author={Azulay, Aharon and Weiss, Yair},
  journal={arXiv preprint arXiv:1805.12177},
  year={2018}
}

@online{xiao2017/online,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

@InProceedings{pmlr-v97-simsekli19a,
  title = 	 {A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks},
  author = 	 {Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5827--5837},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/simsekli19a/simsekli19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/simsekli19a.html},
  abstract = 	 {The gradient noise (GN) in the stochastic gradient descent (SGD) algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. This assumption is often made for mathematical convenience, since it enables SGD to be analyzed as a stochastic differential equation (SDE) driven by a Brownian motion. We argue that the Gaussianity assumption might fail to hold in deep learning settings and hence render the Brownian motion-based analyses inappropriate. Inspired by non-Gaussian natural phenomena, we consider the GN in a more general context and invoke the generalized CLT (GCLT), which suggests that the GN converges to a heavy-tailed $\alpha$-stable random variable. Accordingly, we propose to analyze SGD as an SDE driven by a Lévy motion. Such SDEs can incur ?jumps?, which force the SDE transition from narrow minima to wider minima, as proven by existing metastability theory. To validate the $\alpha$-stable assumption, we conduct experiments on common deep learning scenarios and show that in all settings, the GN is highly non-Gaussian and admits heavy-tails. We investigate the tail behavior in varying network architectures and sizes, loss functions, and datasets. Our results open up a different perspective and shed more light on the belief that SGD prefers wide minima.}
}

@article{lee2020finite,
  title={Finite Versus Infinite Neural Networks: an Empirical Study},
  author={Lee, Jaehoon and Schoenholz, Samuel S and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:2007.15801},
  year={2020}
}

@INPROCEEDINGS{Zagoruyko2016WRN,
    author = {Sergey Zagoruyko and Nikos Komodakis},
    title = {Wide Residual Networks},
    booktitle = {BMVC},
    year = {2016}}

@inproceedings{
yaida2018fluctuationdissipation,
title={Fluctuation-dissipation relations for stochastic gradient descent},
author={Sho Yaida},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SkNksoRctQ},
}

@inproceedings{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2937--2947},
  year={2019}
}
@incollection{Chizat2018,
title = {{On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport}},
author = {Chizat, L\'{e}na\"{\i}c and Bach, Francis},
booktitle = {Advances in Neural Information Processing Systems 31},
pages = {3040--3050},
year = {2018},
publisher = {Curran Associates, Inc.},
}

@article{nguyen2019mean,
  title={Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks},
  author={Nguyen, Phan-Minh},
  journal={arXiv preprint arXiv:1902.02880},
  year={2019}
}
@article{Dyer19,
  title={Asymptotics of wide networks from feynman diagrams},
  author={Dyer, Ethan and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:1909.11304},
  year={2019}
}

@inproceedings{
Du2019,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eK3i09YQ},
}

@inproceedings{Allen-Zhu2018,
	title = {A {Convergence} {Theory} for {Deep} {Learning} via {Over}-{Parameterization}},
	url = {http://proceedings.mlr.press/v97/allen-zhu19a.html},
	abstract = {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of wor...},
	language = {en},
	urldate = {2020-09-29},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {242--252},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\AWHFMRLR\\Allen-Zhu et al. - 2019 - A Convergence Theory for Deep Learning via Over-Pa.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\CJIZ8ZNU\\allen-zhu19a.html:text/html}
}


@article{lecun-mnist,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010,
  journal = {},
}




@book{scholkopf2001learning,
  title={Learning with kernels: support vector machines, regularization, optimization, and beyond},
  author={Scholkopf, Bernhard and Smola, Alexander J},
  year={2001},
  publisher={MIT press}
}

@book{gikhman2015theory,
  title={The theory of stochastic processes I},
  author={Gikhman, Iosif I and Skorokhod, Anatoli V},
  year={2015},
  publisher={Springer}
}

@article{saad1995line,
  title={On-line learning in soft committee machines},
  author={Saad, David and Solla, Sara A},
  journal={Physical Review E},
  volume={52},
  number={4},
  pages={4225},
  year={1995},
  publisher={APS}
}

@book{opper2001advanced,
  title={Advanced mean field methods: Theory and practice},
  author={Opper, Manfred and Saad, David},
  year={2001},
  publisher={MIT press}
}

@book{williams2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  volume={2},
  number={3},
  year={2006},
  publisher={MIT Press Cambridge, MA}
}

@article{smola1998connection,
  title={The connection between regularization operators and support vector kernels},
  author={Smola, Alex J and Sch{\"o}lkopf, Bernhard and M{\"u}ller, Klaus-Robert},
  journal={Neural networks},
  volume={11},
  number={4},
  pages={637--649},
  year={1998},
  publisher={Elsevier}
}

@article{stein1999predicting,
  title={Predicting random fields with increasing dense observations},
  author={Stein, Michael L and others},
  journal={The Annals of Applied Probability},
  volume={9},
  number={1},
  pages={242--273},
  year={1999},
  publisher={Institute of Mathematical Statistics}
}
@article{de2020sparsity,
  title={On Sparsity in Overparametrised Shallow ReLU Networks},
  author={de Dios, Jaume and Bruna, Joan},
  journal={arXiv preprint arXiv:2006.10225},
  year={2020}
}

@article{bruna2013invariant,
  title={Invariant scattering convolution networks},
  author={Bruna, Joan and Mallat, St{\'e}phane},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1872--1886},
  year={2013},
  publisher={IEEE}
}

@book{stein2012interpolation,
  title={Interpolation of spatial data: some theory for kriging},
  author={Stein, Michael L},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md and Ali, Mostofa and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@article{luxburg2004distance,
  title={Distance-based classification with Lipschitz functions},
  author={Luxburg, Ulrike von and Bousquet, Olivier},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={Jun},
  pages={669--695},
  year={2004}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@inproceedings{rudi2017generalization,
  title={Generalization properties of learning with random features},
  author={Rudi, Alessandro and Rosasco, Lorenzo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3215--3225},
  year={2017}
}

@inproceedings{mei2019mean,
	title = {Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
	shorttitle = {Mean-field theory of two-layers neural networks},
	url = {http://proceedings.mlr.press/v99/mei19a.html},
	abstract = {We consider learning two layer neural networks using stochastic gradient descent. The mean-field description of this learning dynamics approximates the evolution of the network weights by an evolut...},
	language = {en},
	urldate = {2020-12-30},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
	month = jun,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2388--2464},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\VNYZQZI9\\mei19a.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\UGUDWNS9\\Mei et al. - 2019 - Mean-field theory of two-layers neural networks d.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\U62KYIE2\\mei19a.html:text/html}
}

@inproceedings{pham2021global,
    title={Global Convergence of Three-layer Neural Networks in the Mean Field Regime},
    author={Huy Tuan Pham and Phan-Minh Nguyen},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=KvyxFqZS_D}
}


@article{ikeda2005asymptotic,
  title={An asymptotic statistical analysis of support vector machines with soft margins},
  author={Ikeda, Kazushi and Aoishi, Tsutomu},
  journal={Neural Networks},
  volume={18},
  number={3},
  pages={251--259},
  year={2005},
  publisher={Elsevier}
}

@article{amari1993universal,
  title={A universal theorem on learning curves},
  author={Amari, Shun-Ichi},
  journal={Neural networks},
  volume={6},
  number={2},
  pages={161--166},
  year={1993},
  publisher={Elsevier}
}

@article{amari1992four,
  title={Four types of learning curves},
  author={Amari, Shun-ichi and Fujita, Naotake and Shinomoto, Shigeru},
  journal={Neural Computation},
  volume={4},
  number={4},
  pages={605--618},
  year={1992},
  publisher={MIT Press}
}


@article{geiger21landscape,
title = {Landscape and training regimes in deep learning},
journal = {Physics Reports},
volume = {924},
pages = {1-18},
year = {2021},
note = {Landscape and training regimes in deep learning},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2021.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0370157321001290},
author = {Mario Geiger and Leonardo Petrini and Matthieu Wyart},
keywords = {Deep learning, Jamming, Lazy training, Feature learning, Neural networks, Loss landscape, Curse of dimensionality, Neural tangent kernel},
abstract = {Deep learning algorithms are responsible for a technological revolution in a variety of tasks including image recognition or Go playing. Yet, why they work is not understood. Ultimately, they manage to classify data lying in high dimension – a feat generically impossible due to the geometry of high dimensional space and the associated curse of dimensionality. Understanding what kind of structure, symmetry or invariance makes data such as images learnable is a fundamental challenge. Other puzzles include that (i) learning corresponds to minimizing a loss in high dimension, which is in general not convex and could well get stuck bad minima. (ii) Deep learning predicting power increases with the number of fitting parameters, even in a regime where data are perfectly fitted. In this manuscript, we review recent results elucidating (i, ii) and the perspective they offer on the (still unexplained) curse of dimensionality paradox. We base our theoretical discussion on the (h,α) plane where h controls the number of parameters and α the scale of the output of the network at initialization, and provide new systematic measures of performance in that plane for two common image classification datasets. We argue that different learning regimes can be organized into a phase diagram. A line of critical points sharply delimits an under-parametrized phase from an over-parametrized one. In over-parametrized nets, learning can operate in two regimes separated by a smooth cross-over. At large initialization, it corresponds to a kernel method, whereas for small initializations features can be learnt, together with invariants in the data. We review the properties of these different phases, of the transition separating them and some open questions. Our treatment emphasizes analogies with physical systems, scaling arguments and the development of numerical observables to quantitatively test these results empirically. Practical implications are also discussed, including the benefit of averaging nets with distinct initial weights, or the choice of parameters (h,α) optimizing performance.}
}

@article{Geiger18,
	title = {Jamming transition as a paradigm to understand the loss landscape of deep neural networks},
	volume = {100},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.100.012115},
	doi = {10.1103/PhysRevE.100.012115},
	abstract = {Deep learning has been immensely successful at a variety of tasks, ranging from classification to artificial intelligence. Learning corresponds to fitting training data, which is implemented by descending a very high-dimensional loss function. Understanding under which conditions neural networks do not get stuck in poor minima of the loss, and how the landscape of that loss evolves as depth is increased, remains a challenge. Here we predict, and test empirically, an analogy between this landscape and the energy landscape of repulsive ellipses. We argue that in fully connected deep networks a phase transition delimits the over- and underparametrized regimes where fitting can or cannot be achieved. In the vicinity of this transition, properties of the curvature of the minima of the loss (the spectrum of the Hessian) are critical. This transition shares direct similarities with the jamming transition by which particles form a disordered solid as the density is increased, which also occurs in certain classes of computational optimization and learning problems such as the perceptron. Our analysis gives a simple explanation as to why poor minima of the loss cannot be encountered in the overparametrized regime. Interestingly, we observe that the ability of fully connected networks to fit random data is independent of their depth, an independence that appears to also hold for real data. We also study a quantity Δ which characterizes how well (Δ{\textless}0) or badly (Δ{\textgreater}0) a datum is learned. At the critical point it is power-law distributed on several decades, P+(Δ)∼Δθ for Δ{\textgreater}0 and P−(Δ)∼(−Δ)−γ for Δ{\textless}0, with exponents that depend on the choice of activation function. This observation suggests that near the transition the loss landscape has a hierarchical structure and that the learning dynamics is prone to avalanche-like dynamics, with abrupt changes in the set of patterns that are learned.},
	number = {1},
	urldate = {2020-09-29},
	journal = {Physical Review E},
	author = {Geiger, Mario and Spigler, Stefano and d'Ascoli, Stéphane and Sagun, Levent and Baity-Jesi, Marco and Biroli, Giulio and Wyart, Matthieu},
	month = jul,
	year = {2019},
	note = {Publisher: American Physical Society},
	pages = {012115},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\NX8283BB\\Geiger et al. - 2019 - Jamming transition as a paradigm to understand the.pdf:application/pdf;APS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\VWT9YRFC\\PhysRevE.100.html:text/html}
}

@inproceedings{domingos00,
  title={A unified bias-variance decomposition},
  author={Domingos, Pedro},
  booktitle={Proceedings of 17th International Conference on Machine Learning},
  pages={231--238},
  year={2000}
}


@string{epje = {Eur.\ Phys.\ J E}}

@string{epl = {Europhys.\ Lett.}}

@string{pnas = {Proc.\ Natl.\ Acad.\ Sci.\ U.S.A.}}

@string{prb = {Phys.\ Rev.\ B}}

@string{pre = {Phys.\ Rev.\ E}}

@string{prl = {Phys.\ Rev.\ Lett.}}

@string{rmp = {Rev.\ Mod.\ Phys.}}

@inproceedings{jacot2018neural,
 author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
 title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
 booktitle = {Proceedings of the 32Nd International Conference on Neural Information Processing Systems},
 series = {NIPS'18},
 year = {2018},
 location = {Montr\&\#233;al, Canada},
 pages = {8580--8589},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3327757.3327948},
 acmid = {3327948},
 publisher = {Curran Associates Inc.},
 address = {USA},
} 

@article{jacot2019hessian,
  title={The asymptotic spectrum of the Hessian of DNN throughout training},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1910.02875},
  year={2019}
}

@article{jacot2019hessian2,
  title={The Neural Tangent Kernel describes the Hessian of Overparametrized DNNs},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
  year={2019},
  journal={},
}

@inproceedings{williams1997computing,
  title={Computing with infinite networks},
  author={Williams, Christopher KI},
  booktitle={Advances in neural information processing systems},
  pages={295--301},
  year={1997}
}

@inproceedings{
novak2018bayesian,
title={Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes},
author={Roman Novak and Lechao Xiao and Yasaman Bahri and Jaehoon Lee and Greg Yang and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-dickstein},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1g30j0qF7},
}

@article{Lee2017,
  title={Deep Neural Networks as Gaussian Processes},
  author={Jae Hoon Lee and Yasaman Bahri and Roman Novak and Samuel S. Schoenholz and Jeffrey Pennington and Jascha Sohl-Dickstein},
  journal={ICLR},
  year={2018}
}

@book{Neal1996,
 author = {Neal, Radford M.},
 title = {Bayesian Learning for Neural Networks},
 year = {1996},
 isbn = {0387947248},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
} 

@incollection{Cho2009,
title = {Kernel Methods for Deep Learning},
author = {Youngmin Cho and Lawrence K. Saul},
booktitle = {Advances in Neural Information Processing Systems 22},
pages = {342--350},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf}
}

@inproceedings{matthews2018gaussian,
  title={Gaussian Process Behaviour in Wide Deep Neural Networks},
  author={Alexander G. de G. Matthews and Jiri Hron and Mark Rowland and Richard E. Turner and Zoubin Ghahramani},
  booktitle={International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=H1-nGgWC-},
}

@article{neal2018modern,
  title={A modern take on the bias-variance tradeoff in neural networks},
  author={Neal, Brady and Mittal, Sarthak and Baratin, Aristide and Tantia, Vinayak and Scicluna, Matthew and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  journal={arXiv preprint arXiv:1810.08591},
  year={2018}
}

@article{Spigler18, 
	title = {A jamming transition from under- to over-parametrization affects generalization in deep learning},
	volume = {52},
	issn = {1751-8121},
	url = {https://iopscience.iop.org/article/10.1088/1751-8121/ab4c8b/meta},
	doi = {10.1088/1751-8121/ab4c8b},
	language = {en},
	number = {47},
	urldate = {2020-09-29},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	author = {Spigler, S. and Geiger, M. and d’Ascoli, S. and Sagun, L. and Biroli, G. and Wyart, M.},
	month = oct,
	year = {2019},
	note = {Publisher: IOP Publishing},
	pages = {474001},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\7FXTMGCL\\Spigler et al. - 2019 - A jamming transition from under- to over-parametri.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\TTZP2QI8\\meta.html:text/html}
}


@article{Zdeborova07,
  title={Phase transitions in the coloring of random graphs},
  author={{Zdeborov{\'a}}, Lenka and Krzakala, Florent},
  journal={Physical Review E},
  volume={76},
  number={3},
  pages={031131},
  year={2007},
  publisher={APS}
}

@article{Krzakala07,
  title={Landscape analysis of constraint satisfaction problems},
  author={Krzakala, Florent and Kurchan, Jorge},
  journal={Physical Review E},
  volume={76},
  number={2},
  pages={021122},
  year={2007},
  publisher={APS}
}

@article{Monasson95,
  title={Weight space structure and internal representations: a direct approach to learning and generalization in multilayer neural networks},
  author={Monasson, {R{\'e}mi} and Zecchina, Riccardo},
  journal={Physical review letters},
  volume={75},
  number={12},
  pages={2432},
  year={1995},
  publisher={APS}
}

@article{Cooper18,
  title={The loss landscape of overparameterized neural networks},
  author={Cooper, Yaim},
  journal={arXiv preprint arXiv:1804.10200},
  year={2018}
}

@inproceedings{eldan2016power,
  title={The power of depth for feedforward neural networks},
  author={Eldan, Ronen and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={907--940},
  year={2016}
}



@article{Li18,
  title={Measuring the intrinsic dimension of objective landscapes},
  author={Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  journal={arXiv preprint arXiv:1804.08838},
  year={2018}
}

@article{Lerner12,
	Author = {E. Lerner and G. {D\"uring} and M. Wyart},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-09-05 15:59:57 +0000},
	Journal = {EPL (Europhysics Letters)},
	Number = {5},
	Pages = {58003},
	Title = {Toward a microscopic description of flow near the jamming threshold},
	Volume = {99},
	Year = {2012},
	Bdsk-Url-1 = {http://stacks.iop.org/0295-5075/99/i=5/a=58003}}
	
@article{Charbonneau12,
	Author = {Charbonneau, Patrick and Corwin, Eric I. and Parisi, Giorgio and Zamponi, Francesco},
	Date = {2012/11/13/},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:33 +0000},
	Day = {13},
	Id = {10.1103/PhysRevLett.109.205501},
	J1 = {PRL},
	Journal = {Physical Review Letters},
	Journal1 = {Phys. Rev. Lett.},
	Month = {11},
	Number = {20},
	Pages = {205501--},
	Publisher = {American Physical Society},
	Title = {Universal Microstructure and Mechanical Stability of Jammed Packings},
	Ty = {JOUR},
	Url = {http://link.aps.org/doi/10.1103/PhysRevLett.109.205501},
	Volume = {109},
	Year = {2012},
	Bdsk-Url-1 = {http://link.aps.org/doi/10.1103/PhysRevLett.109.205501}
}

@misc{yang2019tensor1,
    title={Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes},
    author={Greg Yang},
    year={2019},
    eprint={1910.12478},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}

@misc{yang2020tensor2,
    title={Tensor Programs II: Neural Tangent Kernel for Any Architecture},
    author={Greg Yang},
    year={2020},
    eprint={2006.14548},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{yang2021tensor2b,
    title={Tensor Programs IIb: Architectural Universality of Neural Tangent Kernel Training Dynamics},
    author={Greg Yang and Etai Littwin},
    year={2021},
    eprint={2105.03703},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{yang2020tensor3,
    title={Tensor Programs III: Neural Matrix Laws},
    author={Greg Yang},
    year={2020},
    eprint={2009.10685},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}

@article{yang2019scaling,
  title={Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:1902.04760},
  year={2019}
}

@incollection{lee2019wide,
	title = {Wide {Neural} {Networks} of {Any} {Depth} {Evolve} as {Linear} {Models} {Under} {Gradient} {Descent}},
	url = {http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf},
	urldate = {2020-09-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8572--8583},
	file = {NIPS Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\9XQ8IXMY\\Lee et al. - 2019 - Wide Neural Networks of Any Depth Evolve as Linear.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\DHGXIH9D\\9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.html:text/html}
}

@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}
@article{rotskoff2018neural,
  title={Neural networks as Interacting Particle Systems: Asymptotic convexity of the Loss Landscape and Universal Scaling of the Approximation Error},
  author={Rotskoff, Grant M and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:1805.00915},
  year={2018}
}

@article{ghorbani2019linearized,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={arXiv preprint arXiv:1904.12191},
  year={2019}
}

@article{ongie2019function,
	title = {A {Function} {Space} {View} of {Bounded} {Norm} {Infinite} {Width} {ReLU} {Nets}: {The} {Multivariate} {Case}},
	shorttitle = {A {Function} {Space} {View} of {Bounded} {Norm} {Infinite} {Width} {ReLU} {Nets}},
	url = {https://iclr.cc/virtual_2020/poster_H1lNPxHKDH.html},
	abstract = {We give a tight characterization of the (vectorized Euclidean) norm of weights required to realize a function \$f:{\textbackslash}mathbb\{R\}{\textbackslash}rightarrow {\textbackslash}mathbb\{R\}{\textasciicircum}d\$ as a single hidden-layer ReLU network with an unbounded number of units (infinite width), extending the univariate characterization of Savarese et al. (2019) to the multivariate case.},
	journal = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
	language = {en},
	urldate = {2020-12-30},
	author = {Ongie, Greg and Willett, Rebecca and Soudry, Daniel and Srebro, Nathan},
	month = apr,
	year = {2020},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\LCTGXRBW\\poster_H1lNPxHKDH.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\US3XSNVQ\\Ongie et al. - 2020 - A Function Space View of Bounded Norm Infinite Wid.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\TV9PTDNU\\poster_H1lNPxHKDH.html:text/html}
}

@article{Lerner13a,
	Abstract = {We study theoretically and numerically how hard frictionless particles in random packings can rearrange. We demonstrate the existence of two distinct unstable non-linear modes of rearrangement{,} both associated with the opening and the closing of contacts. The first mode{,} whose density is characterized by some exponent [small theta][prime or minute]{,} corresponds to motions of particles extending throughout the entire system. The second mode{,} whose density is characterized by an exponent [small theta] [not equal] [small theta][prime or minute]{,} corresponds to the local buckling of a few particles. Extended modes are shown to yield at a much higher rate than local modes when a stress is applied. We show that the distribution of contact forces follows P(f) [similar] fmin([small theta][prime or minute]{,}[small theta]){,} and that imposing the restriction that the packing cannot be densified further leads to the bounds and {,} where [gamma] characterizes the singularity of the pair distribution function g(r) at contact. These results extend the theoretical analysis of [Wyart{,} Phys. Rev. Lett.{,} 2012{,} 109{,} 125502] where the existence of local modes was not considered. We perform numerics that support that these bounds are saturated with [gamma] [approximate] 0.38{,} [small theta] [approximate] 0.17 and [small theta][prime or minute] [approximate] 0.44. We measure systematically the stability of all such modes in packings{,} and confirm their marginal stability. The principle of marginal stability thus allows us to make clearcut predictions on the ensemble of configurations visited in these out-of-equilibrium systems{,} and on the contact forces and pair distribution functions. It also reveals the excitations that need to be included in a description of plasticity or flow near jamming{,} and suggests a new path to study two-level systems and soft spots in simple amorphous solids of repulsive particles.},
	Author = {Lerner, Edan and During, Gustavo and Wyart, Matthieu},
	Date-Added = {2014-05-22 20:15:23 +0000},
	Date-Modified = {2014-09-05 16:00:04 +0000},
	Doi = {10.1039/C3SM50515D},
	Issue = {34},
	Journal = {Soft Matter},
	Pages = {8252-8263},
	Publisher = {The Royal Society of Chemistry},
	Title = {Low-energy non-linear excitations in sphere packings},
	Volume = {9},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1039/C3SM50515D}}

@article{Saxe13,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={International Conference on Learning Representations},
  year={2014}
}

@article{Kingma14,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={International Conference on Learning Representations},
  year={2015}
}

@article{Ohern03,
	Author = {O'Hern, Corey S. and Silbert, Leonardo E. and Liu, Andrea J. and Nagel, Sidney R.},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-11-09 08:34:09 +0000},
	Doi = {10.1103/PhysRevE.68.011306},
	Journal = {Phys. Rev. E},
	Month = {Jul},
	Number = {1},
	Pages = {011306--011324},
	Publisher = {American Physical Society},
	Title = {Jamming at zero temperature and zero applied stress: The epitome of disorder},
	Volume = {68},
	Year = {2003},
	Bdsk-Url-1 = {http://link.aps.org/doi/10.1103/PhysRevE.68.011306},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevE.68.011306}}

@article{Balduzzi17,
  title={The Shattered Gradients Problem: If resnets are the answer, then what is the question?},
  author={Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
  journal={arXiv preprint arXiv:1702.08591},
  year={2017}
}

@article{Balduzzi16,
  title={Deep online convex optimization with gated games},
  author={Balduzzi, David},
  journal={arXiv preprint arXiv:1604.01952},
  year={2016}
}

@book{group_georgi,
    Author = {Georgi, H.},
    Year = {2000},
    Title = {Lie Algebras In Particle Physics: from Isospin To Unified Theories},
    Publisher = {CRC Press}
}

@book{group_zee,
    Author = {Zee, A.},
    Year = {2016},
    Title = {Group Theory in a Nutshell for Physicists},
    Publisher = {Princeton University Press}
}

@book{group_dresselhaus,
    Author = {Dresselhaus, Mildred S. and Dresselhaus, Gene and Jorio, Ado},
    Year = {2008},
    Title = {Group Theory},
    Publisher = {Springer},
}

@book{group_serre,
    Author = {Serre, Jean-Pierre},
    Year = {1977},
    Title = {Linear Representations of Finite Groups},
    Publisher = {Springer},
}

@book{Phillips81,
	Author = {Anderson, A.C.},
	Date-Added = {2014-06-13 22:47:59 +0000},
	Date-Modified = {2015-06-04 02:59:24 +0000},
	Editor = {W. A. Phillips},
	Publisher = {Springer, Berlin},
	Series = {Topics in Current Physics},
	Title = {Amorphous Solids: Low Temperature Properties},
	Volume = {24},
	Year = {1981}
}

@article{During13,
	Author = {{D{\"u}ring}, Gustavo and Lerner, Edan and Wyart, Matthieu},
	Date = {2013},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:33 +0000},
	Journal = {Soft Matter},
	Number = {1},
	Pages = {146-154},
	Publisher = {Royal Society of Chemistry},
	Title = {Phonon gap and localization lengths in floppy materials},
	Volume = {9},
	Year = {2013}}

@article{DeGiuli14,
	Author = {DeGiuli, Eric and Laversanne-Finot, Adrien and {D\"uring}, Gustavo Alberto and Lerner, Edan and Wyart, Matthieu},
	Date = {2014},
	Date-Added = {2014-07-08 08:27:13 +0000},
	Date-Modified = {2014-07-10 08:54:49 +0000},
	Journal = {Soft Matter},
	Number = {30},
	Pages = {5628-5644},
	Publisher = {Royal Society of Chemistry},
	Title = {Effects of coordination and pressure on sound attenuation, boson peak and elasticity in amorphous solids},
	Volume = {10},
	Year = {2014}}

@article{Yan16,
	Author = {Yan, Le and DeGiuli, Eric and Wyart, Matthieu},
	Date = {2016},
	Date-Added = {2016-10-13 12:49:09 +0000},
	Date-Modified = {2016-10-13 12:49:27 +0000},
	Journal = {EPL (Europhysics Letters)},
	Number = {2},
	Pages = {26003},
	Publisher = {IOP Publishing},
	Title = {On variational arguments for vibrational modes near jamming},
	Volume = {114},
	Year = {2016}}


@article{Tkachenko99,
	Author = {Tkachenko, Alexei V. and Witten, Thomas A.},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:34 +0000},
	Doi = {10.1103/PhysRevE.60.687},
	Journal = {Phys. Rev. E},
	Month = {Jul},
	Number = {1},
	Numpages = {9},
	Pages = {687--696},
	Publisher = {American Physical Society},
	Title = {Stress propagation through frictionless granular material},
	Volume = {60},
	Year = {1999},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevE.60.687}}

@article{Baum88,
  title={On the capabilities of multilayer perceptrons},
  author={Baum, Eric B},
  journal={Journal of complexity},
  volume={4},
  number={3},
  pages={193--215},
  year={1988},
  publisher={Academic Press}
}

@article{Gardner88,
  title={The space of interactions in neural network models},
  author={Gardner, Elizabeth},
  journal={Journal of physics A: Mathematical and general},
  volume={21},
  number={1},
  pages={257},
  year={1988},
  publisher={IOP Publishing}
}

@article{Franz15,
  title={Universal spectrum of normal modes in low-temperature glasses},
  author={Franz, Silvio and Parisi, Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={47},
  pages={14539--14544},
  year={2015},
  publisher={National Acad Sciences}
}

@article{Franz17,
  title={Universality of the SAT-UNSAT (jamming) threshold in non-convex continuous constraint satisfaction problems},
  author={Franz, Silvio and Parisi, Giorgio and Sevelev, Maxime and Urbani, Pierfrancesco and Zamponi, Francesco},
  journal={SciPost Physics},
  volume={2},
  number={3},
  pages={019},
  year={2017}
}

@inproceedings{
s.2018spherical,
title={Spherical {CNN}s},
author={Taco S. Cohen and Mario Geiger and Jonas Köhler and Max Welling},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Hkbd5xZRb},
}

@inproceedings{NEURIPS2018_488e4104,
 author = {Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data},
 url = {https://proceedings.neurips.cc/paper/2018/file/488e4104520c6aab692863cc1dba45af-Paper.pdf},
 volume = {31},
 year = {2018}
}


@inproceedings{NEURIPS2019_b9cfe8b6,
 author = {Cohen, Taco S and Geiger, Mario and Weiler, Maurice},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A General Theory of Equivariant CNNs on Homogeneous Spaces},
 url = {https://proceedings.neurips.cc/paper/2019/file/b9cfe8b6042cf759dc4c0cccb27a6737-Paper.pdf},
 volume = {32},
 year = {2019}
}


@article{Brito18a,
	title = {Universality of jamming of nonspherical particles},
	volume = {115},
	copyright = {© 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/46/11736},
	doi = {10.1073/pnas.1812457115},
	abstract = {Amorphous packings of nonspherical particles such as ellipsoids and spherocylinders are known to be hypostatic: The number of mechanical contacts between particles is smaller than the number of degrees of freedom, thus violating Maxwell’s mechanical stability criterion. In this work, we propose a general theory of hypostatic amorphous packings and the associated jamming transition. First, we show that many systems fall into a same universality class. As an example, we explicitly map ellipsoids into a system of “breathing” particles. We show by using a marginal stability argument that in both cases jammed packings are hypostatic and that the critical exponents related to the contact number and the vibrational density of states are the same. Furthermore, we introduce a generalized perceptron model which can be solved analytically by the replica method. The analytical solution predicts critical exponents in the same hypostatic jamming universality class. Our analysis further reveals that the force and gap distributions of hypostatic jamming do not show power-law behavior, in marked contrast to the isostatic jamming of spherical particles. Finally, we confirm our theoretical predictions by numerical simulations.},
	language = {en},
	number = {46},
	urldate = {2020-09-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brito, Carolina and Ikeda, Harukuni and Urbani, Pierfrancesco and Wyart, Matthieu and Zamponi, Francesco},
	month = nov,
	year = {2018},
	pmid = {30381457},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {glass, jamming, marginal stability, nonspherical particles},
	pages = {11736--11741},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\URYRABRI\\Brito et al. - 2018 - Universality of jamming of nonspherical particles.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\PHCPDT94\\11736.html:text/html}
}

@article{Muller14,
	Author = {M{\"u}ller, Markus and Wyart, Matthieu},
	Date-Added = {2015-01-05 15:07:48 +0000},
	Date-Modified = {2015-06-04 20:14:00 +0000},
	Doi = {10.1146/annurev-conmatphys-031214-014614},
	Journal = {Annual Review of Condensed Matter Physics},
	Number = {1},
	Pages = {177--200},
	Title = {Marginal Stability in Structural, Spin, and Electron Glasses},
	Volume = {6},
	Year = {2015},
	Bdsk-Url-1 = {http://www.annualreviews.org/doi/abs/10.1146/annurev-conmatphys-031214-014614},
	Bdsk-Url-2 = {http://dx.doi.org/10.1146/annurev-conmatphys-031214-014614}}

@article{Charbonneau15,
	Author = {Charbonneau, Patrick and Corwin, Eric I and Parisi, Giorgio and Zamponi, Francesco},
	Date = {2015},
	Date-Added = {2015-04-16 02:52:40 +0000},
	Date-Modified = {2015-04-16 02:52:40 +0000},
	Journal = {Physical Review Letters},
	Number = {12},
	Pages = {125504},
	Publisher = {APS},
	Title = {Jamming Criticality Revealed by Removing Localized Buckling Excitations},
	Volume = {114},
	Year = {2015}}

@article{Charbonneau14,
	Author = {Charbonneau, Patrick and Kurchan, Jorge and Parisi, Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
	Date = {2014},
	Date-Added = {2014-05-09 02:04:58 +0000},
	Date-Modified = {2014-06-13 16:56:16 +0000},
	Journal = {Nature Communications},
	Number = {3725},
	Publisher = {Nature Publishing Group},
	Title = {Fractal free energy landscapes in structural glasses},
	Volume = {5},
	Year = {2014}}

@article{Charbonneau14a,
	Author = {Charbonneau, Patrick and Kurchan, Jorge and Parisi, Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
	Date = {2014},
	Date-Added = {2014-10-20 02:26:59 +0000},
	Date-Modified = {2014-10-20 02:27:25 +0000},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Number = {10},
	Pages = {10009},
	Publisher = {IOP Publishing},
	Title = {Exact theory of dense amorphous hard spheres in high dimension. III. The full replica symmetry breaking solution},
	Volume = {2014},
	Year = {2014}}
	
@article{Wyart12,
	Author = {Wyart, Matthieu},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-09-05 15:59:36 +0000},
	Doi = {10.1103/PhysRevLett.109.125502},
	Issue = {12},
	Journal = {Phys. Rev. Lett.},
	Month = {Sep},
	Numpages = {5},
	Pages = {125502},
	Publisher = {American Physical Society},
	Title = {Marginal Stability Constrains Force and Pair Distributions at Random Close Packing},
	Volume = {109},
	Year = {2012},
	Bdsk-Url-1 = {http://link.aps.org/doi/10.1103/PhysRevLett.109.125502},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevLett.109.125502}}

@article{Zeravcic09,
	Abstract = {We study the vibrational modes of three-dimensional jammed packings of soft ellipsoids of revolution as a function of particle aspect ratio {{\OE}µ} and packing fraction. At the jamming transition for ellipsoids, as distinct from the idealized case using spheres where {{\OE}µ=1}, there are many unconstrained and nontrivial rotational degrees of freedom. These constitute a set of zero-frequency modes that are gradually mobilized into a new rotational band as {|{\OE}µ-1|} increases. Quite surprisingly, as this new band is separated from zero frequency by a gap, and lies below the onset frequency for translational vibrations, {{\oe}{\^a} * }, the presence of these new degrees of freedom leaves unaltered the basic scenario that the translational spectrum is determined only by the average contact number. Indeed, {{\oe}{\^a} *} depends solely on coordination as it does for compressed packings of spheres. We also discuss the regime of large {|{\OE}µ-1|}, where the two bands merge.},
	Author = {Z. Zeravcic and N. Xu and A. J. Liu and S. R. Nagel and W. van Saarloos},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:34 +0000},
	Journal = EPL,
	Number = {2},
	Pages = {26001},
	Title = {Excitations of ellipsoid packings near jamming},
	Volume = {87},
	Year = {2009}}


@article{Donev04a,
	Abstract = {Packing problems, such as how densely objects can fill a volume, are among the most ancient and persistent problems in mathematics and science. For equal spheres, it has only recently been proved that the face-centered cubic lattice has the highest possible packing fraction . It is also well known that certain random (amorphous) jammed packings have {{\oe}{\"U}} ?{{\^a}{\`a}} 0.64. Here, we show experimentally and with a new simulation algorithm that ellipsoids can randomly pack more densely?{{\"A}{\^\i}}up to {{\oe}{\"U}}= 0.68 to 0.71for spheroids with an aspect ratio close to that of M&M's Candies?{{\"A}{\^\i}}and even approach {{\oe}{\"U}} ?{{\^a}{\`a}} 0.74 for ellipsoids with other aspect ratios. We suggest that the higher density is directly related to the higher number of degrees of freedom per particle and thus the larger number of particle contacts required to mechanically stabilize the packing. We measured the number of contacts per particle Z ?{{\^a}{\`a}} 10 for our spheroids, as compared to Z ?{{\^a}{\`a}} 6 for spheres. Our results have implications for a broad range of scientific disciplines, including the properties of granular media and ceramics, glass formation, and discrete geometry.},
	Author = {Donev, Aleksandar and Cisse, Ibrahim and Sachs, David and Variano, Evan A. and Stillinger, Frank H. and Connelly, Robert and Torquato, Salvatore and Chaikin, P. M.},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:33 +0000},
	Doi = {10.1126/science.1093010},
	Journal = {Science},
	Number = {5660},
	Pages = {990-993},
	Title = {Improving the Density of Jammed Disordered Packings Using Ellipsoids},
	Volume = {303},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1126/science.1093010}}

@article{Franz16,
  title={The simplest model of jamming},
  author={Franz, Silvio and Parisi, Giorgio},
  journal={Journal of Physics A: Mathematical and Theoretical},
  volume={49},
  number={14},
  pages={145001},
  year={2016},
  publisher={IOP Publishing}
}



@article{Mailman09,
	Author = {Mailman, Mitch and Schreck, Carl F. and O'Hern, Corey S. and Chakraborty, Bulbul},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:33 +0000},
	Doi = {10.1103/PhysRevLett.102.255501},
	Journal = {Phys. Rev. Lett.},
	Month = {Jun},
	Number = {25},
	Numpages = {4},
	Pages = {255501},
	Publisher = {American Physical Society},
	Title = {Jamming in Systems Composed of Frictionless Ellipse-Shaped Particles},
	Volume = {102},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.102.255501}}

@article{Silbert05,
	Author = {L. E. Silbert and A. J. Liu and S. R. Nagel},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:34 +0000},
	Journal = PRL,
	Pages = {098301},
	Title = {Vibrations and Diverging Length Scales Near the Unjamming Transition},
	Volume = {95},
	Year = {2005}}
	
@article{Wyart05a,
	Author = {Wyart, Matthieu and Silbert, Leonardo E and Nagel, Sidney R and Witten, Thomas A},
	Date = {2005},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:34 +0000},
	Journal = {Physical Review E},
	Number = {5},
	Pages = {051306},
	Publisher = {APS},
	Title = {Effects of compression on the vibrational modes of marginally jammed solids},
	Volume = {72},
	Year = {2005}}

@article{Lipton16,
  title={Stuck in a what? adventures in weight space},
  author={Lipton, Zachary C},
  journal={International Conference on Learning Representations},
  year={2016}
}

@article{franz2018jamming,
	title = {Jamming in {Multilayer} {Supervised} {Learning} {Models}},
	volume = {123},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.123.160602},
	doi = {10.1103/PhysRevLett.123.160602},
	abstract = {Critical jamming transitions are characterized by an astonishing degree of universality. Analytic and numerical evidence points to the existence of a large universality class that encompasses finite and infinite dimensional spheres and continuous constraint satisfaction problems (CCSP) such as the nonconvex perceptron and related models. In this Letter we investigate multilayer neural networks (MLNN) learning random associations as models for CCSP that could potentially define different jamming universality classes. As opposed to simple perceptrons and infinite dimensional spheres, which are described by a single effective field in terms of which the constraints appear to be one dimensional, the description of MLNN involves multiple fields, and the constraints acquire a multidimensional character. We first study the models numerically and show that similarly to the perceptron, whenever jamming is isostatic, the sphere universality class is recovered, we then write the exact mean-field equations for the models and identify a dimensional reduction mechanism that leads to a scaling regime identical to the one of spheres.},
	number = {16},
	urldate = {2020-09-29},
	journal = {Physical Review Letters},
	author = {Franz, Silvio and Hwang, Sungmin and Urbani, Pierfrancesco},
	month = oct,
	year = {2019},
	note = {Publisher: American Physical Society},
	pages = {160602},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\BLJF3H85\\Franz et al. - 2019 - Jamming in Multilayer Supervised Learning Models.pdf:application/pdf;APS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\EH69D4MZ\\PhysRevLett.123.html:text/html}
}

@article{Franz17b,
  title={Mean-field avalanches in jammed spheres},
  author={Franz, Silvio and Spigler, Stefano},
  journal={Physical Review E},
  volume={95},
  number={2},
  pages={022139},
  year={2017},
  publisher={APS}
}

@article{geiger2019disentangling,
	title = {Disentangling feature and lazy training in deep neural networks},
	volume = {2020},
	issn = {1742-5468},
	url = {https://doi.org/10.1088/1742-5468/abc4de},
	doi = {10.1088/1742-5468/abc4de},
	abstract = {Two distinct limits for deep learning have been derived as the network width h → ∞, depending on how the weights of the last layer scale with h. In the neural tangent Kernel (NTK) limit, the dynamics becomes linear in the weights and is described by a frozen kernel Θ (the NTK). By contrast, in the mean-field limit, the dynamics can be expressed in terms of the distribution of the parameters associated with a neuron, that follows a partial differential equation. In this work we consider deep networks where the weights in the last layer scale as αh −1/2 at initialization. By varying α and h, we probe the crossover between the two limits. We observe two the previously identified regimes of ‘lazy training’ and ‘feature training’. In the lazy-training regime, the dynamics is almost linear and the NTK barely changes after initialization. The feature-training regime includes the mean-field formulation as a limiting case and is characterized by a kernel that evolves in time, and thus learns some features. We perform numerical experiments on MNIST, Fashion-MNIST, EMNIST and CIFAR10 and consider various architectures. We find that: (i) the two regimes are separated by an α* that scales as . (ii) Network architecture and data structure play an important role in determining which regime is better: in our tests, fully-connected networks perform generally better in the lazy-training regime, unlike convolutional networks. (iii) In both regimes, the fluctuations δF induced on the learned function by initial conditions decay as , leading to a performance that increases with h. The same improvement can also be obtained at an intermediate width by ensemble-averaging several networks that are trained independently. (iv) In the feature-training regime we identify a time scale , such that for t ≪ t 1 the dynamics is linear. At t ∼ t 1, the output has grown by a magnitude and the changes of the tangent kernel {\textbar} {\textbar}ΔΘ{\textbar} {\textbar} become significant. Ultimately, it follows for ReLU and Softplus activation functions, with a {\textless} 2 and a → 2 as depth grows. We provide scaling arguments supporting these findings.},
	language = {en},
	number = {11},
	urldate = {2020-12-30},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Geiger, Mario and Spigler, Stefano and Jacot, Arthur and Wyart, Matthieu},
	month = nov,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {113301},
	file = {Submitted Version:C\:\\Users\\leope\\Zotero\\storage\\FCSUFGWL\\Geiger et al. - 2020 - Disentangling feature and lazy training in deep ne.pdf:application/pdf}
}


@article{spigler2019asymptotic,
	title = {Asymptotic learning curves of kernel methods: empirical data versus teacher–student paradigm},
	volume = {2020},
	issn = {1742-5468},
	shorttitle = {Asymptotic learning curves of kernel methods},
	url = {https://doi.org/10.1088/1742-5468/abc61d},
	doi = {10.1088/1742-5468/abc61d},
	abstract = {How many training data are needed to learn a supervised task? It is often observed that the generalization error decreases as n −β where n is the number of training examples and β is an exponent that depends on both data and algorithm. In this work we measure β when applying kernel methods to real datasets. For MNIST we find β ≈ 0.4 and for CIFAR10 β ≈ 0.1, for both regression and classification tasks, and for Gaussian or Laplace kernels. To rationalize the existence of non-trivial exponents that can be independent of the specific kernel used, we study the teacher–student framework for kernels. In this scheme, a teacher generates data according to a Gaussian random field, and a student learns them via kernel regression. With a simplifying assumption—namely that the data are sampled from a regular lattice—we derive analytically β for translation invariant kernels, using previous results from the kriging literature. Provided that the student is not too sensitive to high frequencies, β depends only on the smoothness and dimension of the training data. We confirm numerically that these predictions hold when the training points are sampled at random on a hypersphere. Overall, the test error is found to be controlled by the magnitude of the projection of the true function on the kernel eigenvectors whose rank is larger than n. Using this idea we predict the exponent β from real data by performing kernel PCA, leading to β ≈ 0.36 for MNIST and β ≈ 0.07 for CIFAR10, in good agreement with observations. We argue that these rather large exponents are possible due to the small effective dimension of the data.},
	language = {en},
	number = {12},
	urldate = {2020-12-30},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Spigler, Stefano and Geiger, Mario and Wyart, Matthieu},
	month = dec,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {124001}
}



@InProceedings{Baity18,
  title = 	 {Comparing Dynamics: Deep Neural Networks versus Glassy Systems},
  author = 	 {Baity-Jesi, Marco and Sagun, Levent and Geiger, Mario and Spigler, Stefano and Arous, Gerard Ben and Cammarota, Chiara and LeCun, Yann and Wyart, Matthieu and Biroli, Giulio},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {314--323},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/baity-jesi18a/baity-jesi18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/baity-jesi18a.html},
  abstract = 	 {We analyze numerically the training dynamics of deep neural networks (DNN) by using methods developed in statistical physics of glassy systems. The two main issues we address are the complexity of the loss-landscape and of the dynamics within it, and to what extent DNNs share similarities with glassy systems. Our findings, obtained for different architectures and data-sets, suggest that during the training process the dynamics slows down because of an increasingly large number of flat directions. At large times, when the loss is approaching zero, the system diffuses at the bottom of the landscape. Despite some similarities with the dynamics of mean-field glassy systems, in particular, the absence of barrier crossing, we find distinctive dynamical behaviors in the two cases, thus showing that the statistical properties of the corresponding loss and energy landscapes are different. In contrast, when the network is under-parametrized we observe a typical glassy behavior, thus suggesting the existence of different phases depending on whether the network is under-parametrized or over-parametrized.}
}

@article{Ballard17,
  title={Energy landscapes for machine learning},
  author={Ballard, Andrew J and Das, Ritankar and Martiniani, Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson, Jacob D and Wales, David J},
  journal={Physical Chemistry Chemical Physics},
  year={2017},
  publisher={Royal Society of Chemistry}
}


@article{Sagun16,
  title={Singularity of the Hessian in Deep Learning},
  author={Sagun, Levent and Bottou, {L{\'e}on} and LeCun, Yann},
  journal={International Conference on Learning Representations},
  year={2017}
}

@article{Soudry2016,
  title={No bad local minima: Data independent training error guarantees for multilayer neural networks},
  author={Soudry, Daniel and Carmon, Yair},
  journal={arXiv preprint arXiv:1605.08361},
  year={2016}
}

@inproceedings{Hoffer17,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1729--1739},
  year={2017}
}

@article{Freeman16,
  title={Topology and Geometry of Deep Rectified Network Optimization Landscapes},
  author={Freeman, C Daniel and Bruna, Joan},
  journal={International Conference on Learning Representations},
  year={2017}
}

@article{Hochreiter97,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, {J{\"u}rgen}},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press}
}

@article{Chaudhari16,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={arXiv preprint arXiv:1611.01838},
  year={2016}
}

@article{Achille17,
  title={Emergence of invariance and disentangling in deep representations},
  author={Achille, Alessandro and Soatto, Stefano},
  journal={arXiv preprint arXiv:1706.01350},
  year={2017}
}

@inproceedings{Montufar14,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{Bianchini14,
  title={On the complexity of neural network classifiers: A comparison between shallow and deep architectures},
  author={Bianchini, Monica and Scarselli, Franco},
  journal={IEEE transactions on neural networks and learning systems},
  volume={25},
  number={8},
  pages={1553--1565},
  year={2014},
  publisher={IEEE}
}

@InProceedings{Raghu16,
  title = 	 {On the Expressive Power of Deep Neural Networks},
  author = 	 {Maithra Raghu and Ben Poole and Jon Kleinberg and Surya Ganguli and Jascha Sohl-Dickstein},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2847--2854},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/raghu17a/raghu17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/raghu17a.html},
  abstract = 	 {We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings show that: (1) The complexity of the computed function grows exponentially with depth (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights (3) Trajectory regularization is a simpler alternative to batch normalization, with the same performance.}
}

@inproceedings{Choromanska15,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Ben Arous, G{\'e}rard and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}



@inproceedings{Ioffe15,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015}
}

@inproceedings{He16,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{Lecun95,
  title={Convolutional networks for images, speech, and time series},
  author={LeCun, Yann and Bengio, Yoshua and others},
  journal={The handbook of brain theory and neural networks},
  volume={3361},
  number={10},
  pages={1995},
  year={1995}
}

@article{Hinton12,
  title={Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  journal={IEEE Signal processing magazine},
  volume={29},
  number={6},
  pages={82--97},
  year={2012},
  publisher={IEEE}
}

@article{Silver16,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{Silver17,
  title={Mastering the game of Go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{Lecun15,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436},
  year={2015},
  publisher={Nature Publishing Group}
}



@article{Wyart05b,
	Author = {M. Wyart},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-11-09 08:15:00 +0000},
	Journal = {Annales de Phys},
	Number = {3},
	Pages = {1--113},
	Title = {On the Rigidity of Amorphous Solids},
	Volume = {30},
	Year = {2005}}

@book{Liu10,
author = {J Liu, Andrea and R Nagel, Sidney and Saarloos, W and Wyart, Matthieu},
year = {2010},
month = {06},
pages = {},
title = {The jamming scenario - an introduction and outlook},
booktitle = {Dynamical Heterogeneities in Glasses, Colloids, and Granular Media},
publisher = {OUP Oxford}
}

@article{brito2018theory,
  title={Theory for Swap Acceleration near the Glass and Jamming Transitions},
  author={Brito, Carolina and Lerner, Edan and Wyart, Matthieu},
  journal={arXiv preprint arXiv:1801.03796},
  year={2018}
}

@book{crank1979mathematics,
  title={The mathematics of diffusion},
  author={Crank, John},
  year={1979},
  publisher={Oxford university press}
}

@article{reviewBCKM,
  title={Out of equilibrium dynamics in spin-glasses and other glassy systems},
  author={Bouchaud, Jean-Philippe and Cugliandolo, Leticia F and Kurchan, Jorge and Mezard, Marc},
  journal={Spin glasses and random fields},
  pages={161--223},
  year={1998},
  publisher={World Scientific, Singapore}
}



@incollection{alexnet,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
}

@incollection{birolileshouches,
  title={Slow relaxations and non-equilibrium dynamics in classical and quantum systems},
  author={Biroli, Giulio},
  editor      = "Thierry Giamarchi, Andrew J. Millis, Olivier Parcollet",
  booktitle   = "Strongly Interacting Quantum Systems Out of Equilibrium",
  publisher   = "Oxford University Press",
  address     = "Oxford",
  year        = 2016,
  pages       = "207-261",
}
@article{benarousj,
  title={Spectral gap estimates in mean field spin glasses},
  author={Ben Arous, {G{\'e}rard} and Jagannath, Aukosh},
  journal={arXiv preprint arXiv:1705.04243},
  year={2017}
}

@article{ninarello2017models,
  title={Models and algorithms for the next generation of glass transition studies},
  author={Ninarello, Andrea and Berthier, Ludovic and Coslovich, Daniele},
  journal={Physical Review X},
  volume={7},
  number={2},
  pages={021039},
  year={2017},
  publisher={APS}
}

@article{mezard2002analytic,
  title={Analytic and algorithmic solution of random satisfiability problems},
  author={M{\'e}zard, Marc and Parisi, Giorgio and Zecchina, Riccardo},
  journal={Science},
  volume={297},
  number={5582},
  pages={812--815},
  year={2002},
  publisher={American Association for the Advancement of Science}
}

@article{monasson1999determining,
  title={Determining computational complexity from characteristic phase transitions},
  author={Monasson, {R{\'e}mi} and Zecchina, Riccardo and Kirkpatrick, Scott and Selman, Bart and Troyansky, Lidror},
  journal={Nature},
  volume={400},
  number={6740},
  pages={133},
  year={1999},
  publisher={Nature Publishing Group}
}

@inproceedings{achlioptas2008algorithmic,
  title={Algorithmic barriers from phase transitions},
  author={Achlioptas, Dimitris and Coja-Oghlan, Amin},
  booktitle={Foundations of Computer Science, 2008. FOCS'08. IEEE 49th Annual IEEE Symposium on},
  pages={793--802},
  year={2008},
  organization={IEEE}
}

@article{zdeborovareview,
  title={Statistical physics of inference: Thresholds and algorithms},
  author={{Zdeborov{\'a}}, Lenka and Krzakala, Florent},
  journal={Advances in Physics},
  volume={65},
  number={5},
  pages={453--552},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{BJREM,
  title={Activated aging dynamics and effective trap model description in the random energy model},
  author={Baity-Jesi, Marco and Biroli, Giulio and Cammarota, Chiara},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2018},
  number={1},
  pages={013301},
  year={2018},
  publisher={IOP Publishing}
}

@article{geiger2019scaling,
	title = {Scaling description of generalization with number of parameters in deep learning},
	volume = {2020},
	issn = {1742-5468},
	url = {https://doi.org/10.1088%2F1742-5468%2Fab633c},
	doi = {10.1088/1742-5468/ab633c},
	abstract = {Supervised deep learning involves the training of neural networks with a large number N of parameters. For large enough N, in the so-called over-parametrized regime, one can essentially fit the training data points. Sparsity-based arguments would suggest that the generalization error increases as N grows past a certain threshold N *. Instead, empirical studies have shown that in the over-parametrized regime, generalization error keeps decreasing with N. We resolve this paradox through a new framework. We rely on the so-called Neural Tangent Kernel, which connects large neural nets to kernel methods, to show that the initialization causes finite-size random fluctuations of the neural net output function f N around its expectation . These affect the generalization error for classification: under natural assumptions, it decays to a plateau value in a power-law fashion ∼N −1/2. This description breaks down at a so-called jamming transition N = N *. At this threshold, we argue that diverges. This result leads to a plausible explanation for the cusp in test error known to occur at N *. Our results are confirmed by extensive empirical observations on the MNIST and CIFAR image datasets. Our analysis finally suggests that, given a computational envelope, the smallest generalization error is obtained using several networks of intermediate sizes, just beyond N *, and averaging their outputs.},
	language = {en},
	number = {2},
	urldate = {2020-09-29},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and d'Ascoli, Stéphane and Biroli, Giulio and Hongler, Clément and Wyart, Matthieu},
	month = feb,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {023401},
	file = {IOP Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\CAAC4TDN\\Geiger et al. - 2020 - Scaling description of generalization with number .pdf:application/pdf}
}

@inproceedings{woodworth2020kernel,
	title = {Kernel and {Rich} {Regimes} in {Overparametrized} {Models}},
	url = {http://proceedings.mlr.press/v125/woodworth20a.html},
	abstract = {A recent line of work studies overparametrized neural networks in the “kernel regime,” i.e. when  during training the network behaves as a kernelized linear predictor, and thus, training with grad...},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
	month = jul,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {3635--3673},
}

@article{ghorbani2020neural,
	title = {When {Do} {Neural} {Networks} {Outperform} {Kernel} {Methods}?},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/a9df2255ad642b923d95503b9a7958d8-Abstract.html},
	language = {en},
	urldate = {2020-12-30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
	year = {2020},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\NTEC7ZLN\\a9df2255ad642b923d95503b9a7958d8-Abstract.html:text/html}
}


@article{sirignano2020mean,
  title={Mean field analysis of neural networks: A central limit theorem},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Stochastic Processes and their Applications},
  volume={130},
  number={3},
  pages={1820--1852},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{ghorbani2019limitations,
  title={Limitations of lazy training of two-layers neural network},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9111--9121},
  year={2019}
}
@inproceedings{scholkopf_kernel_1999,
	title = {Kernel principal component analysis},
	abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
	booktitle = {Advances in {Kernel} {Methods} - {Support} {Vector} {Learning}},
	publisher = {MIT Press},
	author = {Scholkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
	year = {1999},
	pages = {327--352},
	file = {Citeseer - Snapshot:C\:\\Users\\leope\\Zotero\\storage\\GFPN86ZX\\summary.html:text/html;Citeseer - Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\7X3FNTFW\\Scholkopf et al. - 1999 - Kernel principal component analysis.pdf:application/pdf}
}
@article{oymak2019generalization,
  title={Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian},
  author={Oymak, Samet and Fabian, Zalan and Li, Mingchen and Soltanolkotabi, Mahdi},
  journal={arXiv preprint arXiv:1906.05392},
  year={2019}
}
@article{kopitkov2019neural,
	title = {Neural {Spectrum} {Alignment}: {Empirical} {Study}},
	author = {Kopitkov, Dmitry and Indelman, Vadim},
  journal={Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2020},
  year={2020},
  publisher={Springer International Publishing}
}


@article{bach2017breaking,
  title={Breaking the curse of dimensionality with convex neural networks},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={629--681},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{bordelon2020spectrum,
	title = {Spectrum {Dependent} {Learning} {Curves} in {Kernel} {Regression} and {Wide} {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v119/bordelon20a.html},
	abstract = {We derive analytical expressions for the generalization performance of kernel regression as a function of the number of training samples using theoretical methods from Gaussian processes and statis...},
	language = {en},
	urldate = {2020-12-30},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bordelon, Blake and Canatar, Abdulkadir and Pehlevan, Cengiz},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1024--1034},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\Z2UDNMTA\\bordelon20a.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\ENNDME6S\\Bordelon et al. - 2020 - Spectrum Dependent Learning Curves in Kernel Regre.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\IDCPMBSK\\bordelon20a.html:text/html}
}
@article{dacey2000center,
  title={Center surround receptive field structure of cone bipolar cells in primate retina},
  author={Dacey, Dennis and Packer, Orin S and Diller, Lisa and Brainard, David and Peterson, Beth and Lee, Barry},
  journal={Vision research},
  volume={40},
  number={14},
  pages={1801--1811},
  year={2000},
  publisher={Elsevier}
}
@article{mezard2017mean,
  title={Mean-field message-passing equations in the Hopfield model and its generalizations},
  author={M{\'e}zard, Marc},
  journal={Physical Review E},
  volume={95},
  number={2},
  pages={022117},
  year={2017},
  publisher={APS}
}

@article{goldt2019modelling,
	title = {Modeling the {Influence} of {Data} {Structure} on {Learning} in {Neural} {Networks}: {The} {Hidden} {Manifold} {Model}},
	volume = {10},
	shorttitle = {Modeling the {Influence} of {Data} {Structure} on {Learning} in {Neural} {Networks}},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.10.041044},
	doi = {10.1103/PhysRevX.10.041044},
	abstract = {Understanding the reasons for the success of deep neural networks trained using stochastic gradient-based methods is a key open problem for the nascent theory of deep learning. The types of data where these networks are most successful, such as images or sequences of speech, are characterized by intricate correlations. Yet, most theoretical work on neural networks does not explicitly model training data or assumes that elements of each data sample are drawn independently from some factorized probability distribution. These approaches are, thus, by construction blind to the correlation structure of real-world datasets and their impact on learning in neural networks. Here, we introduce a generative model for structured datasets that we call the hidden manifold model. The idea is to construct high-dimensional inputs that lie on a lower-dimensional manifold, with labels that depend only on their position within this manifold, akin to a single-layer decoder or generator in a generative adversarial network. We demonstrate that learning of the hidden manifold model is amenable to an analytical treatment by proving a “Gaussian equivalence property” (GEP), and we use the GEP to show how the dynamics of two-layer neural networks trained using one-pass stochastic gradient descent is captured by a set of integro-differential equations that track the performance of the network at all times. This approach permits us to analyze in detail how a neural network learns functions of increasing complexity during training, how its performance depends on its size, and how it is impacted by parameters such as the learning rate or the dimension of the hidden manifold.},
	number = {4},
	urldate = {2020-12-30},
	journal = {Physical Review X},
	author = {Goldt, Sebastian and Mézard, Marc and Krzakala, Florent and Zdeborová, Lenka},
	month = dec,
	year = {2020},
	note = {Publisher: American Physical Society},
	pages = {041044},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\Q8WILTDG\\Goldt et al. - 2020 - Modeling the Influence of Data Structure on Learni.pdf:application/pdf;APS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\MWCILWIK\\PhysRevX.10.html:text/html}
}
@inproceedings{yehudai2019power,
  title={On the power and limitations of random features for understanding neural networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6598--6608},
  year={2019}
}

@inproceedings{shankar2020neural,
	title = {Neural {Kernels} {Without} {Tangents}},
	url = {http://proceedings.mlr.press/v119/shankar20a.html},
	abstract = {We investigate the connections between neural networks and simple building blocks in kernel space. In particular, using well established feature space tools such as direct sum, averaging, and momen...},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shankar, Vaishaal and Fang, Alex and Guo, Wenshuo and Fridovich-Keil, Sara and Ragan-Kelley, Jonathan and Schmidt, Ludwig and Recht, Benjamin},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {8614--8623},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\FEAJ774U\\Shankar et al. - 2020 - Neural Kernels Without Tangents.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\9HNT3HB8\\shankar20a.html:text/html}
}

@inproceedings{arora2019harnessing,
	title = {Harnessing the {Power} of {Infinitely} {Wide} {Deep} {Nets} on {Small}-data {Tasks}},
	url = {https://openreview.net/forum?id=rkl8sJBYvH},
	abstract = {We verify neural tangent kernel is powerful on small data via experiments on UCI datasets, small CIFAR 10 and low-shot learning on VOC07.},
	language = {en},
	urldate = {2020-12-30},
	author = {Arora, Sanjeev and Du, Simon S. and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong and Yu, Dingli},
	month = sep,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\SNH768II\\Arora et al. - 2019 - Harnessing the Power of Infinitely Wide Deep Nets .pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\LPBQRZVJ\\forum.html:text/html},
}


@incollection{arora2019exact,
	title = {On {Exact} {Computation} with an {Infinitely} {Wide} {Neural} {Net}},
	url = {http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf},
	urldate = {2020-09-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8141--8150},
	file = {NIPS Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\XWQDTKB9\\Arora et al. - 2019 - On Exact Computation with an Infinitely Wide Neura.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\I7IJCV83\\9025-on-exact-computation-with-an-infinitely-wide-neural-net.html:text/html}
}


@article{mei2018mean,
	title = {A mean field view of the landscape of two-layer neural networks},
	volume = {115},
	copyright = {Copyright © 2018 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/33/E7665},
	doi = {10.1073/pnas.1806579115},
	abstract = {Multilayer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires optimizing a nonconvex high-dimensional objective (risk function), a problem that is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the former case, does this happen because local minima are absent or because SGD somehow avoids them? In the latter, why do local minima reached by SGD have good generalization properties? In this paper, we consider a simple case, namely two-layer neural networks, and prove that—in a suitable scaling limit—SGD dynamics is captured by a certain nonlinear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples and show how DD can be used to prove convergence of SGD to networks with nearly ideal generalization error. This description allows for “averaging out” some of the complexities of the landscape of neural networks and can be used to prove a general convergence result for noisy SGD.},
	language = {en},
	number = {33},
	urldate = {2020-09-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
	month = aug,
	year = {2018},
	pmid = {30054315},
	note = {Publisher: National Academy of Sciences
Section: PNAS Plus},
	keywords = {gradient flow, neural networks, partial differential equations, stochastic gradient descent, Wasserstein space},
	pages = {E7665--E7671},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\QKVLBAKV\\Mei et al. - 2018 - A mean field view of the landscape of two-layer ne.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\LQ8XF8DU\\E7665.html:text/html}
}

@article{montanarisemerjian,
  title={Rigorous inequalities between length and time scales in glassy systems},
  author={Montanari, Andrea and Semerjian, Guilhem},
  journal={Journal of statistical physics},
  volume={125},
  number={1},
  pages={23},
  year={2006},
  publisher={Springer}
}

@article{cuku,
  title={Analytical solution of the off-equilibrium dynamics of a long-range spin-glass model},
  author={Cugliandolo, Leticia F and Kurchan, Jorge},
  journal={Physical Review Letters},
  volume={71},
  number={1},
  pages={173},
  year={1993},
  publisher={APS}
}

@incollection{cugliandololeshouches,
  title={Course 7: Dynamics of glassy systems},
  author={Cugliandolo, Leticia F},
  booktitle={Slow Relaxations and nonequilibrium dynamics in condensed matter},
  pages={367--521},
  year={2003},
  publisher={Springer}
}

@book{Mezard87,
  title={Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications},
  author={{M{\'e}zard}, Marc and Parisi, Giorgio and Virasoro, Miguel},
  volume={9},
  year={1987},
  publisher={World Scientific Publishing Company}
}

@article{reviewbray,
  title={Theory of phase-ordering kinetics},
  author={Bray, Alan J},
  journal={Advances in Physics},
  volume={51},
  number={2},
  pages={481--587},
  year={2002},
  publisher={Taylor {\&} Francis}
}

@article{eqBAetal,
  title={Cugliandolo-Kurchan equations for dynamics of Spin-Glasses},
  author={Ben Arous, {G\'erard} and Dembo, Amir and Guionnet, Alice},
  journal={Probability theory and related fields},
  volume={136},
  number={4},
  pages={619--660},
  year={2006},
  publisher={Springer}
}

@article{reviewBB,
  title={Theoretical perspective on the glass transition and amorphous materials},
  author={Berthier, Ludovic and Biroli, Giulio},
  journal={Reviews of Modern Physics},
  volume={83},
  number={2},
  pages={587},
  year={2011},
  publisher={APS}
}

@article{coja2018information,
  title={Information-theoretic thresholds from the cavity method},
  author={Coja-Oghlan, Amin and Krzakala, Florent and Perkins, Will and Zdeborova, Lenka},
  journal={Advances in Mathematics},
  volume={333},
  pages={694--795},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{dauphin2014identifying,
  title={Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
  author={Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2933--2941},
  year={2014}
}

@article{sagun2014explorations,
  title={Explorations on high dimensional landscapes},
  author={Sagun, Levent and {G\"uney}, V. {U\u{g}ur} and {G{\'{e}}rard} {Ben Arous} and LeCun, Yann},
  journal={International Conference on Learning Representations Workshop Contribution, arXiv:1412.6615},
  year={2015}
}

@article{barbier2017phase,
  title={Phase transitions, optimal errors and optimality of message-passing in generalized linear models},
  author={Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, L{\'e}o and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:1708.03395},
  year={2017}
}


@article{pearlmutter1994fast,
  title={Fast exact multiplication by the Hessian},
  author={Pearlmutter, Barak A},
  journal={Neural computation},
  volume={6},
  number={1},
  pages={147--160},
  year={1994},
  publisher={MIT Press}
}


@inproceedings{watanabe2007almost,
  title={Almost all learning machines are singular},
  author={Watanabe, Sumio},
  booktitle={Foundations of Computational Intelligence, 2007. FOCI 2007. IEEE Symposium on},
  pages={383--388},
  year={2007},
  organization={IEEE}
}

@article{panageas2016gradient,
  title={Gradient descent only converges to minimizers: Non-isolated critical points and invariant regions},
  author={Panageas, Ioannis and Piliouras, Georgios},
  journal={arXiv preprint arXiv:1605.00405},
  year={2016}
}


@article{lee2016gradient,
  title={Gradient descent converges to minimizers},
  author={Lee, Jason D and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  journal={University of California, Berkeley},
  volume={1050},
  pages={16},
  year={2016}
}

@article{hardt2015train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  journal={arXiv preprint arXiv:1509.01240},
  year={2015}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, {J{\"u}rgen}},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press}
}

@article{keskar2016large,
  title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{nocedal2006numerical,
  title={Numerical Optimization, Second Edition},
  author={Nocedal, Jorge and Wright, Stephen J},
  journal={Numerical optimization},
  pages={497--528},
  year={2006},
  publisher={Springer New York}
}

@incollection{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, {L{\'e}on}},
  booktitle={Proceedings of COMPSTAT'2010},
  pages={177--186},
  year={2010},
  publisher={Physica-Verlag HD}
}

@article{mei2016landscape,
  title={The landscape of empirical risk for non-convex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  journal={arXiv preprint arXiv:1607.06534},
  year={2016}
}

@article{schaul2013no,
  title={No more pesky learning rates.},
  author={Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  journal={ICML (3)},
  volume={28},
  pages={343--351},
  year={2013}
}

@article{bottou1991stochastic,
  title={Stochastic gradient learning in neural networks},
  author={Bottou, {L{\'e}on}},
  journal={Proceedings of Neuro-{N{\i}mes}},
  volume={91},
  number={8},
  year={1991}
}

@article{bourrely1989parallelization,
  title={Parallelization of a neural network learning algorithm on a hypercube},
  author={Bourrely, J},
  journal={Hypercube and distributed computers. Elsiever Science Publishing},
  year={1989}
}

@article{auffinger2013random,
  title={Random matrices and complexity of spin glasses},
  author={Auffinger, Antonio and Ben Arous, {G{\'e}rard} and {\v{C}}ern{\`y}, Ji{\v{r}}{\'\i}},
  journal={Communications on Pure and Applied Mathematics},
  volume={66},
  number={2},
  pages={165--201},
  year={2013},
  publisher={Wiley Online Library}
}

@article{kurchanlaloux,
  title={Phase space geometry and slow dynamics},
  author={Kurchan, Jorge and Laloux, Laurent},
  journal={Journal of Physics A: Mathematical and General},
  volume={29},
  number={9},
  pages={1929},
  year={1996},
  publisher={IOP Publishing},
}

@article {pnasmontanariksat,
	author = {{Krzaka{\l}a}, Florent and Montanari, Andrea and Ricci-Tersenghi, Federico and Semerjian, Guilhem and {Zdeborov{\'a}}, Lenka},
	title = {Gibbs states and the set of solutions of random constraint satisfaction problems},
	volume = {104},
	number = {25},
	pages = {10318--10323},
	year = {2007},
	doi = {10.1073/pnas.0703685104},
	publisher = {National Academy of Sciences},
	abstract = {An instance of a random constraint satisfaction problem defines a random subset ?? (the set of solutions) of a large product space X N (the set of assignments). We consider two prototypical problem ensembles (random k-satisfiability and q-coloring of random regular graphs) and study the uniform measure with support on S. As the number of constraints per variable increases, this measure first decomposes into an exponential number of pure states ({\textquotedblleft}clusters{\textquotedblright}) and subsequently condensates over the largest such states. Above the condensation point, the mass carried by the n largest states follows a Poisson-Dirichlet process. For typical large instances, the two transitions are sharp. We determine their precise location. Further, we provide a formal definition of each phase transition in terms of different notions of correlation between distinct variables in the problem. The degree of correlation naturally affects the performances of many search/sampling algorithms. Empirical evidence suggests that local Monte Carlo Markov chain strategies are effective up to the clustering phase transition and belief propagation up to the condensation point. Finally, refined message passing techniques (such as survey propagation) may also beat this threshold.},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences},
}

@article{cavagnaSGpedestrians,
  title={Spin-glass theory for pedestrians},
  author={Castellani, Tommaso and Cavagna, Andrea},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2005},
  number={05},
  pages={P05012},
  year={2005},
  publisher={IOP Publishing},
}

@article{ballard2017perspective,
  title={Perspective: Energy Landscapes for Machine Learning},
  author={Ballard, Andrew J and Das, Ritankar and Martiniani, Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson, Jacob D and Wales, David J},
  journal={arXiv preprint arXiv:1703.07915},
  year={2017}
}

@article{piela1989multiple,
  title={On the multiple-minima problem in the conformational analysis of molecules: deformation of the potential energy hypersurface by the diffusion equation method},
  author={Piela, Lucjan and Kostrowicki, Jaroslaw and Scheraga, Harold A},
  journal={The Journal of Physical Chemistry},
  volume={93},
  number={8},
  pages={3339--3346},
  year={1989},
  publisher={ACS Publications}
}

@inproceedings{mobahi2015theoretical,
  title={A Theoretical Analysis of Optimization by Gaussian Continuation.},
  author={Mobahi, Hossein and Fisher III, John W},
  booktitle={AAAI},
  pages={1205--1211},
  year={2015},
  organization={Citeseer}
}

@inproceedings{mobahi2015link,
  title={On the link between gaussian homotopy continuation and convex envelopes},
  author={Mobahi, Hossein and Fisher III, John W},
  booktitle={International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition},
  pages={43--56},
  year={2015},
  organization={Springer}
}

@article{pardalos1994optimization,
  title={Optimization methods for computing global minima of nonconvex potential energy functions},
  author={Pardalos, Panos M and Shalloway, David and Xue, Guoliang},
  journal={Journal of Global Optimization},
  volume={4},
  number={2},
  pages={117--133},
  year={1994},
  publisher={Springer}
}

@article{bengio2009learning,
  title={Learning deep architectures for AI},
  author={Bengio, Yoshua and others},
  journal={Foundations and trends{\textregistered} in Machine Learning},
  volume={2},
  number={1},
  pages={1--127},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@article{gulcehre2016mollifying,
  title={Mollifying Networks},
  author={Gulcehre, Caglar and Moczulski, Marcin and Visin, Francesco and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1608.04980},
  year={2016}
}

@article{baldassi2016unreasonable,
  title = {Unreasonable effectiveness of learning neural networks: {From} accessible states and robust ensembles to basic algorithmic schemes},
  volume = {113},
  issn = {0027-8424, 1091-6490},
  shorttitle = {Unreasonable effectiveness of learning neural networks},
  doi = {10.1073/pnas.1608103113},
  language = {en},
  number = {48},
  urldate = {2016-11-30},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer T. and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
  month = nov,
  year = {2016},
  pages = {E7655--E7662},
}

@inproceedings{aubin2018committee,
  title={The committee machine: Computational to statistical gaps in learning a two-layers neural network},
  author={Aubin, Benjamin and Maillard, Antoine and Krzakala, Florent and Macris, Nicolas and Zdeborov{\'a}, Lenka and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3223--3234},
  year={2018}
}

@inproceedings{gabrie2018entropy,
  title={Entropy and mutual information in models of deep neural networks},
  author={Gabri{\'e}, Marylou and Manoel, Andre and Luneau, Cl{\'e}ment and Macris, Nicolas and Krzakala, Florent and Zdeborov{\'a}, Lenka and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1821--1831},
  year={2018}
}

@article{chaudhari2016entropy,
  title={Entropy-SGD: Biasing Gradient Descent Into Wide Valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={arXiv preprint arXiv:1611.01838},
  year={2016}
}

@inproceedings{park2019effect,
	title = {The {Effect} of {Network} {Width} on {Stochastic} {Gradient} {Descent} and {Generalization}: an {Empirical} {Study}},
	shorttitle = {The {Effect} of {Network} {Width} on {Stochastic} {Gradient} {Descent} and {Generalization}},
	url = {http://proceedings.mlr.press/v97/park19b.html},
	abstract = {We investigate how the final parameters found by stochastic gradient descent are influenced by over-parameterization. We generate families of models by increasing the number of channels in a base n...},
	language = {en},
	urldate = {2020-12-30},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Park, Daniel and Sohl-Dickstein, Jascha and Le, Quoc and Smith, Samuel},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {5042--5051},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\ZLMSGD5R\\park19b.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\652UXKDY\\Park et al. - 2019 - The Effect of Network Width on Stochastic Gradient.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\MGMYLNWN\\park19b.html:text/html}
}
@inproceedings{hazan2016graduated,
  title={On graduated optimization for stochastic non-convex problems},
  author={Hazan, Elad and Levy, Kfir Yehuda and Shalev-Shwartz, Shai},
  booktitle={International Conference on Machine Learning},
  pages={1833--1841},
  year={2016}
}

@article{horn1962eigenvalues,
  title={Eigenvalues of sums of Hermitian matrices},
  author={Horn, Alfred},
  journal={Pacific Journal of Mathematics},
  volume={12},
  number={1},
  pages={225--241},
  year={1962},
  publisher={Mathematical Sciences Publishers}
}

@article{thompson1971eigenvalues,
  title={On the eigenvalues of sums of Hermitian matrices},
  author={Thompson, Robert C and Freede, Linda J},
  journal={Linear Algebra and Its Applications},
  volume={4},
  number={4},
  pages={369--376},
  year={1971},
  publisher={Elsevier}
}

@article{knutson2001honeycombs,
  title={Honeycombs and sums of Hermitian matrices},
  author={Knutson, Allen and Tao, Terence},
  journal={Notices Amer. Math. Soc},
  volume={48},
  number={2},
  year={2001}
}

@article{marvcenko1967distribution,
  title={Distribution of eigenvalues for some sets of random matrices},
  author={{Mar{\v{c}}enko}, Vladimir A and Pastur, Leonid Andreevich},
  journal={Mathematics of the USSR-Sbornik},
  volume={1},
  number={4},
  pages={457},
  year={1967},
  publisher={IOP Publishing}
}

@article{bloemendal2016principal,
  title={On the principal components of sample covariance matrices},
  author={Bloemendal, Alex and Knowles, Antti and Yau, Horng-Tzer and Yin, Jun},
  journal={Probability Theory and Related Fields},
  volume={164},
  number={1-2},
  pages={459--552},
  year={2016},
  publisher={Springer}
}

@article{mobahi2016training,
  title={Training Recurrent Neural Networks by Diffusion},
  author={Mobahi, Hossein},
  journal={arXiv preprint arXiv:1601.04114},
  year={2016}
}

@article{baik2005phase,
  title={Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices},
  author={Baik, Jinho and Ben Arous, {G{\'e}rard} and {P{\'e}ch{\'e}}, Sandrine and others},
  journal={The Annals of Probability},
  volume={33},
  number={5},
  pages={1643--1697},
  year={2005},
  publisher={Institute of Mathematical Statistics}
}

@article{moller1993exact,
  title={Exact Calculation of the Product of the Hessian Matrix of Feed-Forward Network Error Functions and a Vector in 0 (N) Time},
  author={{M{\o}ller}, Martin F},
  journal={DAIMI Report Series},
  volume={22},
  number={432},
  year={1993}
}

@article{hassibi1993second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David G and others},
  journal={Advances in neural information processing systems},
  pages={164--164},
  year={1993},
  publisher={Morgan Kaufmann Publishers}
}

@article{dinh2017sharp,
  title={Sharp Minima Can Generalize For Deep Nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1703.04933},
  year={2017}
}

@incollection{lecun90,
title = {Optimal Brain Damage},
author = {LeCun, Yann and John S. Denker and Sara A. Solla},
booktitle = {Advances in Neural Information Processing Systems 2},
editor = {D. S. Touretzky},
pages = {598--605},
year = {1990},
publisher = {Morgan-Kaufmann},
url = {http://papers.nips.cc/paper/250-optimal-brain-damage.pdf}
}

@software{autograd15,
  author = {Dougal Maclaurin and David Duvenaud and Matthew Johnson and Ryan P. Adams},
  title = {Autograd: Reverse-mode differentiation of native {P}ython},
  url = {http://github.com/HIPS/autograd},
  version = {1.1.2},
  year = {2015},
}


@inproceedings{han15a,
  title={Learning both Weights and Connections for Efficient Neural Network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  pages={1135--1143},
  year={2015}
}


@article{han15b,
  title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={International Conference on Learning Representations (ICLR)},
  year={2016}
}

@article{wen16,
  author    = {Wei Wen and
               Chunpeng Wu and
               Yandan Wang and
               Yiran Chen and
               Hai Li},
  title     = {Learning Structured Sparsity in Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1608.03665},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.03665},
  timestamp = {Mon, 30 Jan 2017 17:08:13 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/WenWWCL16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@INPROCEEDINGS{liu2015, 
    author={Baoyuan Liu and Min Wang and H. Foroosh and M. Tappen and M. Penksy}, 
    booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
    title={Sparse Convolutional Neural Networks}, 
    year={2015}, 
    pages={806-814}, 
    keywords={matrix decomposition;matrix multiplication;neural nets;object detection;SCNN model;cascade model;object detection problem;sparse convolutional neural networks;sparse decomposition;sparse fully connected layers;sparse matrix multiplication algorithm;Accuracy;Convolutional codes;Kernel;Matrix decomposition;Neural networks;Redundancy;Sparse matrices},
    doi={10.1109/CVPR.2015.7298681}, 
    ISSN={1063-6919}, 
    month={June},
}
@inproceedings{denton14,
 author = {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
 title = {Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems},
 series = {NIPS'14},
 year = {2014},
 location = {Montreal, Canada},
 pages = {1269--1277},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2968826.2968968},
 acmid = {2968968},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 


@article{lecun1998efficient,
  title={Efficient backprop},
  author={LeCun, Yann and Bottou, {L\'eon} and Orr, GB and {M{\"u}ller}, K-R},
  journal={Lecture notes in computer science},
  pages={9--50},
  year={1998},
  publisher={Springer}
}
@inproceedings{li2019towards,
  title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11674--11685},
  year={2019}
}
@article{shwartz2017opening,
  title={Opening the Black Box of Deep Neural Networks via Information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal={arXiv preprint arXiv:1703.00810},
  year={2017}
}

@article{goyal2017accurate,
  title={Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
  author={Goyal, Priya and {Doll{\'a}r}, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@inproceedings{martens2010deep,
  title={Deep learning via Hessian-free optimization},
  author={Martens, James},
  booktitle={Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  pages={735--742},
  year={2010}
}

@article{jastrzkebski2017three,
  title={Three Factors Influencing Minima in SGD},
  author={Jastrzebski, {Stanis{\l}aw} and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}





@article{sagun2017empirical,
  title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},
  author={Sagun, Levent and Evci, Utku and {G\"uney}, V. {U\u{g}ur} and Dauphin, Yann and Bottou, {L\'eon}},
  journal={ICLR 2018 Workshop Contribution, arXiv:1706.04454},
  year={2017}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, {L{\'e}on} and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}



@inproceedings{kawaguchi2016deep,
  title={Deep learning without poor local minima},
  author={Kawaguchi, Kenji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={586--594},
  year={2016}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}



@article{krishnan2017neumann,
  title={Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks},
  author={Krishnan, Shankar and Xiao, Ying and Saurous, Rif A},
  journal={arXiv preprint arXiv:1712.03298},
  year={2017}
}

@article{Gastaldi17,
  title={Shake-Shake regularization of 3-branch residual networks},
  author={Gastaldi, Xavier},
  journal={International Conference on Learning Representations},
  year={2017}
}



@InProceedings{lee2017ability,
  title = 	 {On the Ability of Neural Nets to Express Distributions},
  author = 	 {Holden Lee and Rong Ge and Tengyu Ma and Andrej Risteski and Sanjeev Arora},
  booktitle = 	 {Proceedings of the 2017 Conference on Learning Theory},
  pages = 	 {1271--1296},
  year = 	 {2017},
  editor = 	 {Satyen Kale and Ohad Shamir},
  volume = 	 {65},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Amsterdam, Netherlands},
  month = 	 {07--10 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v65/lee17a/lee17a.pdf},
  url = 	 {http://proceedings.mlr.press/v65/lee17a.html},
  abstract = 	 {Deep neural nets have caused a revolution in many classification tasks. A related ongoing revolution?also theoretically not understood?concerns their ability to serve as generative models for complicated types of data such as images and texts. These models are trained using ideas like variational autoencoders and Generative Adversarial Networks. We take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with $n$ hidden layers. A key ingredient is Barron?s Theorem (Barron, 1993), which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of $n$ functions which satisfy certain Fourier conditions (?Barron functions?) can be approximated by a $n+1$-layer neural network. For probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance?a natural metric on probability distributions?by a neural network applied to a fixed base distribution (e.g., multivariate gaussian). Building up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone.}
}

@book{engel2001statistical,
  title={Statistical mechanics of learning},
  author={Engel, Andreas and Van den Broeck, Christian},
  year={2001},
  publisher={Cambridge University Press}
}

@incollection{prechelt1998early,
  title={Early stopping-but when?},
  author={Prechelt, Lutz},
  booktitle={Neural Networks: Tricks of the trade},
  pages={55--69},
  year={1998},
  publisher={Springer}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{caruana2001overfitting,
  title={Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping},
  author={Caruana, Rich and Lawrence, Steve and Giles, C Lee},
  booktitle={Advances in neural information processing systems},
  pages={402--408},
  year={2001}
}

@inproceedings{krogh1992simple,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John A},
  booktitle={Advances in neural information processing systems},
  pages={950--957},
  year={1992}
}

@inproceedings{haeffele2017global,
  title={Global optimality in neural network training},
  author={Haeffele, Benjamin D and Vidal, {Ren{\'e}}},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7331--7339},
  year={2017}
}

@article{liao2018dynamics,
  title={The Dynamics of Learning: A Random Matrix Approach},
  author={Liao, Zhenyu and Couillet, Romain},
  journal={arXiv preprint arXiv:1805.11917},
  year={2018}
}

@article{neyshabur2017geometry,
  title={Geometry of optimization and implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Salakhutdinov, Ruslan and Srebro, Nathan},
  journal={arXiv preprint arXiv:1705.03071},
  year={2017}
}

@article{neyshabur2018towards,
  title={Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={arXiv preprint arXiv:1805.12076},
  year={2018}
}

@article{venturi2018neural,
  title={Neural Networks with Finite Intrinsic Dimension have no Spurious Valleys},
  author={Venturi, Luca and Bandeira, Afonso and Bruna, Joan},
  journal={arXiv preprint arXiv:1802.06384},
  year={2018}
}

@article{advani2017high,
	title = {High-dimensional dynamics of generalization error in neural networks},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608020303117},
	doi = {10.1016/j.neunet.2020.08.022},
	abstract = {We perform an analysis of the average generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant “high-dimensional” regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that standard application of theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
	language = {en},
	urldate = {2020-09-29},
	journal = {Neural Networks},
	author = {Advani, Madhu S. and Saxe, Andrew M. and Sompolinsky, Haim},
	month = sep,
	year = {2020},
	keywords = {Generalization error, Neural networks, Random matrix theory},
	file = {ScienceDirect Snapshot:C\:\\Users\\leope\\Zotero\\storage\\E87WVE3H\\S0893608020303117.html:text/html;Submitted Version:C\:\\Users\\leope\\Zotero\\storage\\ZAZZFPTJ\\Advani et al. - 2020 - High-dimensional dynamics of generalization error .pdf:application/pdf}
}

@article{monasson1995weight,
  title={Weight space structure and internal representations: a direct approach to learning and generalization in multilayer neural networks},
  author={Monasson, R{\'e}mi and Zecchina, Riccardo},
  journal={Physical review letters},
  volume={75},
  number={12},
  pages={2432},
  year={1995},
  publisher={APS}
}

@article{bansal2018minnorm,
  title={Minnorm training: an algorithm for training overcomplete deep neural networks},
  author={Bansal, Yamini and Advani, Madhu and Cox, David D and Saxe, Andrew M},
  journal={arXiv preprint arXiv:1806.00730},
  year={2018}
}

@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  year={1990}
}

@article{matheron1963principles,
  title={Principles of geostatistics},
  author={Matheron, Georges},
  journal={Economic geology},
  volume={58},
  number={8},
  pages={1246--1266},
  year={1963},
  publisher={Society of Economic Geologists}
}

@inproceedings{mahajan2018exploring,
  title={Exploring the limits of weakly supervised pretraining},
  author={Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and van der Maaten, Laurens},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={181--196},
  year={2018}
}

@article{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and {Jastrz{\k{e}}bski}, {Stanis{\l}aw} and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  journal={arXiv preprint arXiv:1706.05394},
  year={2017}
}

@article{laurent2017multilinear,
  title={The Multilinear Structure of ReLU Networks},
  author={Laurent, Thomas and von Brecht, James},
  journal={arXiv preprint arXiv:1712.10132},
  year={2017}
}

@article{urban2016deep,
  title={Do deep convolutional nets really need to be deep and convolutional?},
  author={Urban, Gregor and Geras, Krzysztof J and Kahou, Samira Ebrahimi and Aslan, Ozlem and Wang, Shengjie and Caruana, Rich and Mohamed, Abdelrahman and Philipose, Matthai and Richardson, Matt},
  journal={arXiv preprint arXiv:1603.05691},
  year={2016}
}


@inproceedings{bos1997dynamics,
  title={Dynamics of training},
  author={{B{\"o}s}, Siegfried and Opper, Manfred},
  booktitle={Advances in Neural Information Processing Systems},
  pages={141--147},
  year={1997}
}

@article{le1991eigenvalues,
  title={Eigenvalues of covariance matrices: Application to neural-network learning},
  author={Le Cun, Yann and Kanter, Ido and Solla, Sara A},
  journal={Physical Review Letters},
  volume={66},
  number={18},
  pages={2396},
  year={1991},
  publisher={APS}
}

@article{sirignano2018mean,
	title = {Mean {Field} {Analysis} of {Neural} {Networks}: {A} {Law} of {Large} {Numbers}},
	volume = {80},
	issn = {0036-1399},
	shorttitle = {Mean {Field} {Analysis} of {Neural} {Networks}},
	url = {https://epubs.siam.org/doi/abs/10.1137/18M1192184},
	doi = {10.1137/18M1192184},
	abstract = {Machine learning, and in particular neural network models, have revolutionized fields such as image, text, and speech recognition. Today, many important real-world applications in these areas are driven by neural networks. There are also growing applications in engineering, robotics, medicine, and finance. Despite their immense success in practice, there is limited mathematical understanding of neural networks. This paper illustrates how neural networks can be studied via stochastic analysis and develops approaches for addressing some of the technical challenges which arise. We analyze one-layer neural networks in the asymptotic regime of simultaneously (a) large network sizes and (b) large numbers of stochastic gradient descent training iterations. We rigorously prove that the empirical distribution of the neural network parameters converges to the solution of a nonlinear partial differential equation. This result can be considered a law of large numbers for neural networks. In addition, a consequence of our analysis is that the trained parameters of the neural network asymptotically become independent, a property which is commonly called “propagation of chaos.”},
	number = {2},
	urldate = {2020-12-30},
	journal = {SIAM Journal on Applied Mathematics},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	month = jan,
	year = {2020},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {725--752},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\YUF9HRL6\\18M1192184.html:text/html;Submitted Version:C\:\\Users\\leope\\Zotero\\storage\\JEB5UDWH\\Sirignano and Spiliopoulos - 2020 - Mean Field Analysis of Neural Networks A Law of L.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\78H6MFXL\\18M1192184.html:text/html}
}

@article{liang2018just,
  title={Just Interpolate: Kernel" Ridgeless" Regression Can Generalize},
  author={Liang, Tengyuan and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:1808.00387},
  year={2018}
}


@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={70},
  year={2018}
}

@article{facco_estimating_2017,
	title = {Estimating the intrinsic dimension of datasets by a minimal neighborhood information},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-11873-y},
	doi = {10.1038/s41598-017-11873-y},
	abstract = {Analyzing large volumes of high-dimensional data is an issue of fundamental importance in data science, molecular simulations and beyond. Several approaches work on the assumption that the important content of a dataset belongs to a manifold whose Intrinsic Dimension (ID) is much lower than the crude large number of coordinates. Such manifold is generally twisted and curved; in addition points on it will be non-uniformly distributed: two factors that make the identification of the ID and its exploitation really hard. Here we propose a new ID estimator using only the distance of the first and the second nearest neighbor of each point in the sample. This extreme minimality enables us to reduce the effects of curvature, of density variation, and the resulting computational cost. The ID estimator is theoretically exact in uniformly distributed datasets, and provides consistent measures in general. When used in combination with block analysis, it allows discriminating the relevant dimensions as a function of the block size. This allows estimating the ID even when the data lie on a manifold perturbed by a high-dimensional noise, a situation often encountered in real world data sets. We demonstrate the usefulness of the approach on molecular simulations and image analysis.},
	language = {en},
	number = {1},
	journal = {Scientific Reports},
	author = {Facco, Elena and d’Errico, Maria and Rodriguez, Alex and Laio, Alessandro},
	month = {9},
	year = {2017},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {12140}
}

@techreport{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly},
	author = {Krizhevsky, Alex},
	year = {2009},
	file = {Citeseer - Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\VH8P8Q54\\summary.html:text/html;Citeseer - Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\88WVCAQC\\Krizhevsky - 2009 - Learning multiple layers of features from tiny ima.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\MJNNCHWK\\summary.html:text/html},
}